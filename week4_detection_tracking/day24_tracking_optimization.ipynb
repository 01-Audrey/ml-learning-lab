{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a7ee2-4a54-4b9f-a73b-d83b083b87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================\n",
    "ML LEARNING JOURNEY - DAY 24\n",
    "==================================================\n",
    "Week: 4 of 24\n",
    "Day: 24 of 168\n",
    "Date: November 19, 2025\n",
    "Topic: Tracking Optimization & People Counting\n",
    "Overall Progress: 14.3%\n",
    "\n",
    "Week 4: Detection & Tracking Foundation\n",
    "‚úÖ Day 22: Project Planning & Architecture (COMPLETED)\n",
    "‚úÖ Day 23: Multi-Object Tracking (DeepSORT) (COMPLETED)\n",
    "üîÑ Day 24: Tracking Optimization (TODAY!)\n",
    "‚¨ú Day 25: Video Processing Pipeline\n",
    "‚¨ú Day 26: Testing & Performance\n",
    "‚¨ú Day 27: Code Cleanup & Modularization\n",
    "‚¨ú Day 28: Week 4 Review\n",
    "\n",
    "Progress: 43% (3/7 days)\n",
    "\n",
    "==================================================\n",
    "üéØ Week 4 Project: Security System - Detection & Tracking\n",
    "- Optimize tracking parameters for security scenarios\n",
    "- Implement people counting (entry/exit)\n",
    "- Handle occlusions and re-identification\n",
    "- Define zones for monitoring\n",
    "- Achieve consistent 30 FPS performance\n",
    "\n",
    "üéØ Today's Learning Objectives:\n",
    "1. Tune DeepSORT parameters (max_age, n_init, IOU threshold)\n",
    "2. Implement line-crossing detection for people counting\n",
    "3. Handle occlusions robustly (maintain IDs through disappearance)\n",
    "4. Create zone-based monitoring (restricted areas)\n",
    "5. Count people entering and exiting specific areas\n",
    "6. Optimize for different scenarios (crowded, sparse, occlusions)\n",
    "7. Measure tracking accuracy and ID consistency\n",
    "\n",
    "üìö Today's Structure:\n",
    "   Part 1 (2h): Parameter Tuning & Occlusion Handling\n",
    "   Part 2 (2h): People Counting System\n",
    "   Part 3 (2h): Zone-Based Monitoring\n",
    "   Part 4 (1h): Testing & Summary\n",
    "\n",
    "üéØ SUCCESS CRITERIA:\n",
    "   ‚úÖ Tracking parameters optimized (tested scenarios)\n",
    "   ‚úÖ Occlusions handled (IDs maintained through disappearance)\n",
    "   ‚úÖ Line-crossing detection working (entry/exit counting)\n",
    "   ‚úÖ People counting accurate (¬±5% error)\n",
    "   ‚úÖ Zone monitoring implemented (restricted areas)\n",
    "   ‚úÖ Direction detection working (in vs out)\n",
    "   ‚úÖ Performance maintained (25-30 FPS)\n",
    "   ‚úÖ Ready for video processing pipeline (Day 25)\n",
    "==================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d90413-8046-44eb-890a-89ba2fb0cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing required libraries...\n",
      "‚è±Ô∏è  This should be quick (most already installed)...\n",
      "\n",
      "Checking ultralytics...\n",
      "Checking deep-sort-realtime...\n",
      "Checking opencv-python...\n",
      "Checking numpy...\n",
      "Checking pandas...\n",
      "Checking matplotlib...\n",
      "Checking scipy...\n",
      "\n",
      "‚úÖ All libraries ready!\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìö IMPORTING LIBRARIES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All libraries imported successfully!\n",
      "\n",
      "üìä Library versions:\n",
      "   ‚Ä¢ OpenCV: 4.12.0\n",
      "   ‚Ä¢ NumPy: 2.2.6\n",
      "   ‚Ä¢ Pandas: 2.3.2\n",
      "   ‚Ä¢ Ultralytics: Installed ‚úì\n",
      "   ‚Ä¢ DeepSORT: Installed ‚úì\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# INSTALL REQUIRED LIBRARIES\n",
    "# ==================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ Installing required libraries...\")\n",
    "print(\"‚è±Ô∏è  This should be quick (most already installed)...\\n\")\n",
    "\n",
    "packages = [\n",
    "    'ultralytics',\n",
    "    'deep-sort-realtime',\n",
    "    'opencv-python',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'scipy'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Checking {package}...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "\n",
    "print(\"\\n‚úÖ All libraries ready!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ==================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìö IMPORTING LIBRARIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "import json\n",
    "\n",
    "# Data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Computer vision\n",
    "import cv2\n",
    "\n",
    "# Deep learning\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Tracking\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
    "print(\"\\nüìä Library versions:\")\n",
    "print(f\"   ‚Ä¢ OpenCV: {cv2.__version__}\")\n",
    "print(f\"   ‚Ä¢ NumPy: {np.__version__}\")\n",
    "print(f\"   ‚Ä¢ Pandas: {pd.__version__}\")\n",
    "print(\"   ‚Ä¢ Ultralytics: Installed ‚úì\")\n",
    "print(\"   ‚Ä¢ DeepSORT: Installed ‚úì\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a235898b-2c77-4d5e-a8c0-8c1f52477c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìö PART 1: PARAMETER TUNING & OCCLUSION HANDLING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìö PART 1: PARAMETER TUNING & OCCLUSION HANDLING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fbf961-638b-4f2c-bca4-21327b6317e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 1.1: Understanding DeepSORT Parameters\n",
      "================================================================================\n",
      "\n",
      "üìä PARAMETER COMPARISON TABLE:\n",
      "\n",
      "Parameter        | Default | Crowded | Sparse  | High Occlusion | Fast Moving\n",
      "-----------------|---------|---------|---------|----------------|------------\n",
      "max_age          |   30    |  20-30  |  15-20  |     40-60      |   10-15\n",
      "n_init           |    3    |   3-4   |   2-3   |      4-5       |     2\n",
      "max_iou_distance |  0.7    | 0.6-0.7 | 0.7-0.8 |    0.5-0.6     |  0.7-0.8\n",
      "max_cosine_dist  |  0.2    |  0.15   |  0.25   |      0.15      |    0.25\n",
      "nn_budget        |  100    |   100   |   50    |      150       |    50\n",
      "\n",
      "Recommendation for Security System (Office/Factory):\n",
      "- Start with: max_age=30, n_init=3, max_iou_distance=0.7\n",
      "- Tune based on specific environment\n",
      "- Test with real footage from deployment location\n",
      "\n",
      "\n",
      "‚úÖ Exercise 1.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 1.1: UNDERSTAND DEEPSORT PARAMETERS\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 1.1: Understanding DeepSORT Parameters\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: DeepSORT Critical Parameters\n",
    "\n",
    "The three most important parameters for tuning:\n",
    "\n",
    "1. max_age (Default: 30)\n",
    "   ‚Ä¢ How many frames to keep a track without detection\n",
    "   ‚Ä¢ Higher = More robust to occlusions (but more false positives)\n",
    "   ‚Ä¢ Lower = Faster deletion (but IDs lost easily)\n",
    "   ‚Ä¢ Formula: max_age = expected_occlusion_time * FPS\n",
    "   ‚Ä¢ Example: 1 second occlusion @ 30fps ‚Üí max_age=30\n",
    "\n",
    "2. n_init (Default: 3)\n",
    "   ‚Ä¢ Consecutive detections needed to confirm a track\n",
    "   ‚Ä¢ Higher = Fewer false tracks (but slower confirmation)\n",
    "   ‚Ä¢ Lower = Faster confirmation (but more false positives)\n",
    "   ‚Ä¢ Typically: 2-5 frames\n",
    "\n",
    "3. max_iou_distance (Default: 0.7)\n",
    "   ‚Ä¢ Threshold for matching based on bounding box overlap\n",
    "   ‚Ä¢ IOU = Intersection over Union\n",
    "   ‚Ä¢ Lower = Stricter matching (fewer ID switches)\n",
    "   ‚Ä¢ Higher = More lenient (more matches, potential errors)\n",
    "   ‚Ä¢ Range: 0.5-0.9\n",
    "\n",
    "Additional Parameters:\n",
    "\n",
    "4. max_cosine_distance (Default: 0.2)\n",
    "   ‚Ä¢ Threshold for appearance matching\n",
    "   ‚Ä¢ Based on ReID embedding similarity\n",
    "   ‚Ä¢ Lower = Objects must look very similar\n",
    "   ‚Ä¢ Higher = More lenient appearance matching\n",
    "\n",
    "5. nn_budget (Default: 100)\n",
    "   ‚Ä¢ Number of appearance features to keep per track\n",
    "   ‚Ä¢ Higher = Better re-identification (but more memory)\n",
    "   ‚Ä¢ Lower = Less memory (but worse re-ID)\n",
    "\n",
    "==================================================\n",
    "\n",
    "PARAMETER TUNING GUIDELINES:\n",
    "\n",
    "Scenario: Crowded Areas (Mall, Station)\n",
    "‚îú‚îÄ‚îÄ max_age: 20-30 (frequent occlusions)\n",
    "‚îú‚îÄ‚îÄ n_init: 3-4 (reduce false positives)\n",
    "‚îú‚îÄ‚îÄ max_iou_distance: 0.6-0.7 (strict matching)\n",
    "‚îî‚îÄ‚îÄ Goal: Reduce ID switches in crowds\n",
    "\n",
    "Scenario: Sparse Areas (Office Hallway)\n",
    "‚îú‚îÄ‚îÄ max_age: 15-20 (people rarely hidden)\n",
    "‚îú‚îÄ‚îÄ n_init: 2-3 (fast confirmation)\n",
    "‚îú‚îÄ‚îÄ max_iou_distance: 0.7-0.8 (lenient)\n",
    "‚îî‚îÄ‚îÄ Goal: Quick tracking, smooth experience\n",
    "\n",
    "Scenario: High Occlusion (Factory, Warehouse)\n",
    "‚îú‚îÄ‚îÄ max_age: 40-60 (objects frequently hidden)\n",
    "‚îú‚îÄ‚îÄ n_init: 4-5 (confirm before showing ID)\n",
    "‚îú‚îÄ‚îÄ max_iou_distance: 0.5-0.6 (very strict)\n",
    "‚îî‚îÄ‚îÄ Goal: Maintain IDs through long occlusions\n",
    "\n",
    "Scenario: Fast Moving Objects (Entrance/Exit)\n",
    "‚îú‚îÄ‚îÄ max_age: 10-15 (people pass quickly)\n",
    "‚îú‚îÄ‚îÄ n_init: 2 (immediate confirmation)\n",
    "‚îú‚îÄ‚îÄ max_iou_distance: 0.7-0.8 (allow fast motion)\n",
    "‚îî‚îÄ‚îÄ Goal: Count people before they leave frame\n",
    "\n",
    "==================================================\n",
    "\n",
    "IMPACT ON PERFORMANCE:\n",
    "\n",
    "Speed Impact:\n",
    "- max_age: Minimal (just checks counter)\n",
    "- n_init: Minimal (just checks counter)\n",
    "- max_iou_distance: None (threshold only)\n",
    "- nn_budget: High budget = more memory, slightly slower\n",
    "- Overall: Parameter tuning doesn't significantly affect FPS\n",
    "\n",
    "Accuracy Impact:\n",
    "- max_age too low ‚Üí Lost tracks frequently\n",
    "- max_age too high ‚Üí False tracks persist\n",
    "- n_init too low ‚Üí Many false positives\n",
    "- n_init too high ‚Üí Slow to confirm real tracks\n",
    "- max_iou_distance too low ‚Üí Missed matches, ID switches\n",
    "- max_iou_distance too high ‚Üí Wrong matches\n",
    "\n",
    "==================================================\n",
    "\n",
    "TESTING METHODOLOGY:\n",
    "\n",
    "1. Start with defaults (max_age=30, n_init=3, iou=0.7)\n",
    "2. Record baseline metrics (ID switches, lost tracks)\n",
    "3. Adjust ONE parameter at a time\n",
    "4. Test on representative videos\n",
    "5. Measure improvement\n",
    "6. Iterate until optimal\n",
    "\n",
    "Metrics to Track:\n",
    "- ID switches per minute\n",
    "- Average track length\n",
    "- False positive tracks\n",
    "- Lost tracks (premature deletion)\n",
    "- Re-identification success rate\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "üìä PARAMETER COMPARISON TABLE:\n",
    "\n",
    "Parameter        | Default | Crowded | Sparse  | High Occlusion | Fast Moving\n",
    "-----------------|---------|---------|---------|----------------|------------\n",
    "max_age          |   30    |  20-30  |  15-20  |     40-60      |   10-15\n",
    "n_init           |    3    |   3-4   |   2-3   |      4-5       |     2\n",
    "max_iou_distance |  0.7    | 0.6-0.7 | 0.7-0.8 |    0.5-0.6     |  0.7-0.8\n",
    "max_cosine_dist  |  0.2    |  0.15   |  0.25   |      0.15      |    0.25\n",
    "nn_budget        |  100    |   100   |   50    |      150       |    50\n",
    "\n",
    "Recommendation for Security System (Office/Factory):\n",
    "- Start with: max_age=30, n_init=3, max_iou_distance=0.7\n",
    "- Tune based on specific environment\n",
    "- Test with real footage from deployment location\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 1.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f95337-6f4a-4cc4-b0f1-a1b46bd44bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 1.2: Create Parameter Testing Framework\n",
      "================================================================================\n",
      "‚úÖ Class created: TrackerConfig\n",
      "   ‚Ä¢ Purpose: Store and manage tracker configurations\n",
      "   ‚Ä¢ Methods: create_tracker(), __repr__()\n",
      "\n",
      "üìã Test Configurations Created:\n",
      "================================================================================\n",
      "   1. Default: max_age=30, n_init=3, iou=0.7\n",
      "   2. High_Occlusion: max_age=50, n_init=4, iou=0.6\n",
      "   3. Fast_Confirmation: max_age=20, n_init=2, iou=0.75\n",
      "   4. Strict_Matching: max_age=30, n_init=3, iou=0.5\n",
      "   5. Lenient_Matching: max_age=30, n_init=3, iou=0.85\n",
      "\n",
      "üí° Testing Strategy:\n",
      "   ‚Ä¢ Run each configuration on same video\n",
      "   ‚Ä¢ Collect metrics for comparison\n",
      "   ‚Ä¢ Select best for security system use case\n",
      "\n",
      "‚úÖ Exercise 1.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 1.2: CREATE PARAMETER TESTING FRAMEWORK\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 1.2: Create Parameter Testing Framework\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Systematic Parameter Testing\n",
    "\n",
    "Why We Need a Framework:\n",
    "- Consistent testing across parameter values\n",
    "- Objective performance comparison\n",
    "- Track metrics over time\n",
    "- Make data-driven decisions\n",
    "\n",
    "Testing Approach:\n",
    "1. Define test video/scenario\n",
    "2. Create multiple tracker configurations\n",
    "3. Run each configuration\n",
    "4. Collect metrics\n",
    "5. Compare and select best\n",
    "\n",
    "Metrics to Collect:\n",
    "- Track count (total unique IDs seen)\n",
    "- Average track duration (frames)\n",
    "- ID switches (when same person gets new ID)\n",
    "- FPS (frames per second)\n",
    "- Lost tracks (tracks deleted prematurely)\n",
    "\"\"\"\n",
    "\n",
    "class TrackerConfig:\n",
    "    \"\"\"Configuration for DeepSORT tracker\"\"\"\n",
    "    \n",
    "    def __init__(self, name, max_age=30, n_init=3, max_iou_distance=0.7):\n",
    "        self.name = name\n",
    "        self.max_age = max_age\n",
    "        self.n_init = n_init\n",
    "        self.max_iou_distance = max_iou_distance\n",
    "    \n",
    "    def create_tracker(self):\n",
    "        \"\"\"Create DeepSORT tracker with this config\"\"\"\n",
    "        return DeepSort(\n",
    "            max_age=self.max_age,\n",
    "            n_init=self.n_init,\n",
    "            max_iou_distance=self.max_iou_distance,\n",
    "            embedder=\"mobilenet\",\n",
    "            embedder_gpu=False\n",
    "        )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: max_age={self.max_age}, n_init={self.n_init}, iou={self.max_iou_distance}\"\n",
    "\n",
    "print(\"‚úÖ Class created: TrackerConfig\")\n",
    "print(\"   ‚Ä¢ Purpose: Store and manage tracker configurations\")\n",
    "print(\"   ‚Ä¢ Methods: create_tracker(), __repr__()\")\n",
    "\n",
    "# Define test configurations\n",
    "configs = [\n",
    "    TrackerConfig(\"Default\", max_age=30, n_init=3, max_iou_distance=0.7),\n",
    "    TrackerConfig(\"High_Occlusion\", max_age=50, n_init=4, max_iou_distance=0.6),\n",
    "    TrackerConfig(\"Fast_Confirmation\", max_age=20, n_init=2, max_iou_distance=0.75),\n",
    "    TrackerConfig(\"Strict_Matching\", max_age=30, n_init=3, max_iou_distance=0.5),\n",
    "    TrackerConfig(\"Lenient_Matching\", max_age=30, n_init=3, max_iou_distance=0.85),\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Test Configurations Created:\")\n",
    "print(\"=\" * 80)\n",
    "for i, config in enumerate(configs, 1):\n",
    "    print(f\"   {i}. {config}\")\n",
    "\n",
    "print(\"\\nüí° Testing Strategy:\")\n",
    "print(\"   ‚Ä¢ Run each configuration on same video\")\n",
    "print(\"   ‚Ä¢ Collect metrics for comparison\")\n",
    "print(\"   ‚Ä¢ Select best for security system use case\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 1.2 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e933cb3d-7036-4873-b32c-45436ae85d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 1.3: Test Occlusion Handling\n",
      "================================================================================\n",
      "\n",
      "üß™ OCCLUSION SIMULATION:\n",
      "================================================================================\n",
      "\n",
      "Scenario: Person walks behind pillar\n",
      "   ‚Ä¢ Visible: Frames 1-10\n",
      "   ‚Ä¢ Occluded: Frames 11-25 (15 frames)\n",
      "   ‚Ä¢ Reappears: Frame 26+\n",
      "\n",
      "üìä Testing with different max_age values:\n",
      "   max_age=10      ‚Üí ‚ùå Track LOST         (max_age=10 < occlusion=15)\n",
      "   max_age=20      ‚Üí ‚úÖ Track MAINTAINED   (max_age=20 > occlusion=15)\n",
      "   max_age=30      ‚Üí ‚úÖ Track MAINTAINED   (max_age=30 > occlusion=15)\n",
      "   max_age=50      ‚Üí ‚úÖ Track MAINTAINED   (max_age=50 > occlusion=15)\n",
      "\n",
      "üí° Key Insight:\n",
      "   ‚Ä¢ max_age must exceed expected occlusion duration\n",
      "   ‚Ä¢ Too low: Frequent ID losses\n",
      "   ‚Ä¢ Too high: Ghost tracks persist longer\n",
      "   ‚Ä¢ Sweet spot: 1.5-2x typical occlusion duration\n",
      "\n",
      "üìà Recommended max_age by Scenario:\n",
      "   ‚Ä¢ Office (rare occlusions): 20-30 frames\n",
      "   ‚Ä¢ Retail (moderate): 30-40 frames\n",
      "   ‚Ä¢ Factory (frequent): 40-60 frames\n",
      "   ‚Ä¢ Outdoor (variable): 30-50 frames\n",
      "\n",
      "‚úÖ Exercise 1.3 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 1.3: OCCLUSION HANDLING TEST\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 1.3: Test Occlusion Handling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Handling Occlusions in Tracking\n",
    "\n",
    "What is Occlusion?\n",
    "- When tracked object is temporarily hidden\n",
    "- Person walks behind pillar\n",
    "- Person temporarily leaves frame\n",
    "- Person overlapped by another person\n",
    "\n",
    "Why Occlusions are Challenging:\n",
    "- No detection = no update to Kalman filter\n",
    "- Predicted position becomes less accurate\n",
    "- Risk of losing track ID\n",
    "- Risk of assigning new ID when reappears\n",
    "\n",
    "How DeepSORT Handles Occlusions:\n",
    "\n",
    "1. Kalman Filter Prediction:\n",
    "   ‚Ä¢ Continues predicting position during occlusion\n",
    "   ‚Ä¢ Uses last known velocity\n",
    "   ‚Ä¢ Prediction uncertainty increases over time\n",
    "\n",
    "2. Track State Management:\n",
    "   ‚Ä¢ Tracked ‚Üí Lost (when no detection)\n",
    "   ‚Ä¢ Lost track kept for max_age frames\n",
    "   ‚Ä¢ If reappears: match by appearance + predicted position\n",
    "   ‚Ä¢ If doesn't reappear: delete track\n",
    "\n",
    "3. Appearance Descriptor:\n",
    "   ‚Ä¢ ReID embedding stored for each track\n",
    "   ‚Ä¢ When object reappears, match by appearance\n",
    "   ‚Ä¢ Even if position prediction is off\n",
    "   ‚Ä¢ Enables re-identification\n",
    "\n",
    "Success Factors:\n",
    "‚úì max_age long enough for expected occlusion\n",
    "‚úì Appearance features distinctive enough\n",
    "‚úì Predicted position reasonably accurate\n",
    "‚úì No similar-looking objects in scene\n",
    "\n",
    "==================================================\n",
    "\n",
    "OCCLUSION SCENARIOS:\n",
    "\n",
    "Short Occlusion (1-10 frames, <0.5 seconds):\n",
    "‚îú‚îÄ‚îÄ Challenge: Low\n",
    "‚îú‚îÄ‚îÄ Solution: Kalman prediction usually sufficient\n",
    "‚îú‚îÄ‚îÄ max_age: 15-20 frames adequate\n",
    "‚îî‚îÄ‚îÄ Success Rate: 95%+\n",
    "\n",
    "Medium Occlusion (10-30 frames, 0.5-1 second):\n",
    "‚îú‚îÄ‚îÄ Challenge: Moderate\n",
    "‚îú‚îÄ‚îÄ Solution: Appearance matching critical\n",
    "‚îú‚îÄ‚îÄ max_age: 30-40 frames needed\n",
    "‚îî‚îÄ‚îÄ Success Rate: 80-90%\n",
    "\n",
    "Long Occlusion (30+ frames, 1+ seconds):\n",
    "‚îú‚îÄ‚îÄ Challenge: High\n",
    "‚îú‚îÄ‚îÄ Solution: May require higher max_age + good appearance\n",
    "‚îú‚îÄ‚îÄ max_age: 50-60+ frames\n",
    "‚îî‚îÄ‚îÄ Success Rate: 60-80%\n",
    "\n",
    "Permanent Occlusion (object leaves):\n",
    "‚îú‚îÄ‚îÄ Challenge: N/A (expected behavior)\n",
    "‚îú‚îÄ‚îÄ Solution: Track should be deleted after max_age\n",
    "‚îú‚îÄ‚îÄ max_age: Balance between persistence and deletion\n",
    "‚îî‚îÄ‚îÄ Goal: Delete cleanly, no ghost tracks\n",
    "\"\"\"\n",
    "\n",
    "def simulate_occlusion_scenario():\n",
    "    \"\"\"\n",
    "    Simulate tracking through occlusion\n",
    "    \n",
    "    Demonstrates how max_age affects track persistence\n",
    "    \"\"\"\n",
    "    print(\"\\nüß™ OCCLUSION SIMULATION:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nScenario: Person walks behind pillar\")\n",
    "    print(\"   ‚Ä¢ Visible: Frames 1-10\")\n",
    "    print(\"   ‚Ä¢ Occluded: Frames 11-25 (15 frames)\")\n",
    "    print(\"   ‚Ä¢ Reappears: Frame 26+\")\n",
    "    \n",
    "    print(\"\\nüìä Testing with different max_age values:\")\n",
    "    \n",
    "    test_configs = [\n",
    "        (\"max_age=10\", 10),\n",
    "        (\"max_age=20\", 20),\n",
    "        (\"max_age=30\", 30),\n",
    "        (\"max_age=50\", 50),\n",
    "    ]\n",
    "    \n",
    "    occlusion_duration = 15  # frames\n",
    "    \n",
    "    for name, max_age in test_configs:\n",
    "        if max_age >= occlusion_duration:\n",
    "            result = \"‚úÖ Track MAINTAINED\"\n",
    "            explanation = f\"(max_age={max_age} > occlusion={occlusion_duration})\"\n",
    "        else:\n",
    "            result = \"‚ùå Track LOST\"\n",
    "            explanation = f\"(max_age={max_age} < occlusion={occlusion_duration})\"\n",
    "        \n",
    "        print(f\"   {name:15} ‚Üí {result:20} {explanation}\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insight:\")\n",
    "    print(\"   ‚Ä¢ max_age must exceed expected occlusion duration\")\n",
    "    print(\"   ‚Ä¢ Too low: Frequent ID losses\")\n",
    "    print(\"   ‚Ä¢ Too high: Ghost tracks persist longer\")\n",
    "    print(\"   ‚Ä¢ Sweet spot: 1.5-2x typical occlusion duration\")\n",
    "    \n",
    "    print(\"\\nüìà Recommended max_age by Scenario:\")\n",
    "    print(\"   ‚Ä¢ Office (rare occlusions): 20-30 frames\")\n",
    "    print(\"   ‚Ä¢ Retail (moderate): 30-40 frames\")\n",
    "    print(\"   ‚Ä¢ Factory (frequent): 40-60 frames\")\n",
    "    print(\"   ‚Ä¢ Outdoor (variable): 30-50 frames\")\n",
    "\n",
    "simulate_occlusion_scenario()\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 1.3 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff2cf99-f46e-4b23-9253-8b79a9f21198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî¢ PART 2: PEOPLE COUNTING SYSTEM\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üî¢ PART 2: PEOPLE COUNTING SYSTEM\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc688ab-beb0-4009-886b-374fc49f89e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 2.1: Understanding Line-Crossing Detection\n",
      "================================================================================\n",
      "\n",
      "üìê VISUAL REPRESENTATION:\n",
      "\n",
      "Line-Crossing Detection:\n",
      "\n",
      "Frame N-1:                Frame N:\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ\n",
      "‚îÇ    ‚óè        ‚îÇ          ‚îÇ             ‚îÇ\n",
      "‚îÇ   P1        ‚îÇ          ‚îÇ      ‚óè      ‚îÇ\n",
      "‚îÇ             ‚îÇ          ‚îÇ     P2      ‚îÇ\n",
      "‚îÇ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ LINE     ‚îÇ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ LINE\n",
      "‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ\n",
      "‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "P1 above line (cross < 0)    P2 below line (cross > 0)\n",
      "                    ‚Üí CROSSING DETECTED! ‚úì\n",
      "\n",
      "Direction Determination:\n",
      "- P1 to P2: Negative to Positive ‚Üí Direction: DOWN (or IN)\n",
      "- P2 to P1: Positive to Negative ‚Üí Direction: UP (or OUT)\n",
      "\n",
      "==================================================\n",
      "\n",
      "IMPLEMENTATION STEPS:\n",
      "\n",
      "1. Define line coordinates\n",
      "   line_start = (x1, y1)\n",
      "   line_end = (x2, y2)\n",
      "\n",
      "2. Track object centers\n",
      "   track_positions[track_id] = [(x, y), ...]\n",
      "\n",
      "3. Calculate cross product\n",
      "   cross_prev = calculate_cross_product(prev_pos, line)\n",
      "   cross_curr = calculate_cross_product(curr_pos, line)\n",
      "\n",
      "4. Detect crossing\n",
      "   if sign(cross_prev) != sign(cross_curr):\n",
      "       crossing_detected = True\n",
      "\n",
      "5. Determine direction\n",
      "   if cross_prev < 0 and cross_curr > 0:\n",
      "       direction = \"ENTERING\"\n",
      "   elif cross_prev > 0 and cross_curr < 0:\n",
      "       direction = \"EXITING\"\n",
      "\n",
      "6. Update counters\n",
      "   if direction == \"ENTERING\":\n",
      "       count_in += 1\n",
      "   elif direction == \"EXITING\":\n",
      "       count_out += 1\n",
      "\n",
      "   current_occupancy = count_in - count_out\n",
      "\n",
      "\n",
      "‚úÖ Exercise 2.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 2.1: UNDERSTAND LINE-CROSSING DETECTION\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 2.1: Understanding Line-Crossing Detection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Line-Crossing for People Counting\n",
    "\n",
    "Concept:\n",
    "- Define a virtual line in the video\n",
    "- Track when objects cross the line\n",
    "- Count direction (entering vs exiting)\n",
    "- Maintain running totals\n",
    "\n",
    "Why Line-Crossing?\n",
    "‚úì Simple and effective\n",
    "‚úì Works with any tracking system\n",
    "‚úì Direction-aware (in vs out)\n",
    "‚úì Handles multiple people simultaneously\n",
    "‚úì Industry standard approach\n",
    "\n",
    "==================================================\n",
    "\n",
    "LINE-CROSSING ALGORITHM:\n",
    "\n",
    "Components Needed:\n",
    "1. Line definition: Two points (x1, y1) and (x2, y2)\n",
    "2. Track positions: Current and previous frame\n",
    "3. Crossing detection: Did trajectory cross line?\n",
    "4. Direction detection: Which way did they cross?\n",
    "\n",
    "Mathematical Approach:\n",
    "\n",
    "Method 1: Line Segment Intersection\n",
    "- Check if line segment (prev_pos, curr_pos) intersects counting line\n",
    "- Use cross product to determine intersection\n",
    "- Determine direction using relative positions\n",
    "\n",
    "Method 2: Signed Distance\n",
    "- Calculate perpendicular distance from point to line\n",
    "- Sign indicates which side of line\n",
    "- Crossing = sign change between frames\n",
    "- Direction = positive to negative vs negative to positive\n",
    "\n",
    "==================================================\n",
    "\n",
    "GEOMETRIC FORMULA:\n",
    "\n",
    "For line from point A(x1, y1) to B(x2, y2)\n",
    "And object center at point P(x, y)\n",
    "\n",
    "Cross Product Method:\n",
    "- v1 = (x2 - x1, y2 - y1)  # Line vector\n",
    "- v2 = (x - x1, y - y1)    # Point vector\n",
    "- cross = v1[0] * v2[1] - v1[1] * v2[0]\n",
    "- cross > 0: Point on one side\n",
    "- cross < 0: Point on other side\n",
    "- cross = 0: Point on line\n",
    "\n",
    "Crossing Detection:\n",
    "- If sign(cross_prev) ‚â† sign(cross_curr): CROSSED!\n",
    "- Direction: cross_prev < 0 and cross_curr > 0 ‚Üí Direction 1\n",
    "- Direction: cross_prev > 0 and cross_curr < 0 ‚Üí Direction 2\n",
    "\n",
    "==================================================\n",
    "\n",
    "LINE PLACEMENT STRATEGIES:\n",
    "\n",
    "Entrance/Exit Monitoring:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        ‚îÇ\n",
    "‚îÇ     ‚Üì‚Üì‚Üì  ENTRANCE  ‚Üë‚Üë‚Üë ‚îÇ\n",
    "‚îÇ     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ  ‚Üê Counting line\n",
    "‚îÇ                        ‚îÇ\n",
    "‚îÇ      MONITORED AREA    ‚îÇ\n",
    "‚îÇ                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Placement Tips:\n",
    "‚úì Place perpendicular to traffic flow\n",
    "‚úì Avoid areas with frequent stops\n",
    "‚úì Clear sightline (no obstructions)\n",
    "‚úì Adequate lighting\n",
    "‚úì Consider camera angle\n",
    "\n",
    "Multiple Lines:\n",
    "- Line 1: Main entrance\n",
    "- Line 2: Secondary entrance\n",
    "- Line 3: Exit-only door\n",
    "- Total = sum of all lines\n",
    "\n",
    "==================================================\n",
    "\n",
    "COUNTING SCENARIOS:\n",
    "\n",
    "Scenario 1: Single Entrance/Exit (Bidirectional)\n",
    "- One line, count both directions\n",
    "- IN: crosses line one way\n",
    "- OUT: crosses line other way\n",
    "- Net count = IN - OUT\n",
    "\n",
    "Scenario 2: Separate Entrance/Exit\n",
    "- Two lines (one per door)\n",
    "- Line 1: Only count crossings in\n",
    "- Line 2: Only count crossings out\n",
    "- Total IN, Total OUT tracked separately\n",
    "\n",
    "Scenario 3: Zone Counting\n",
    "- Define polygon zone\n",
    "- Count entries (any line crossing into zone)\n",
    "- Count exits (any line crossing out of zone)\n",
    "- Current occupancy = entries - exits\n",
    "\n",
    "Scenario 4: Bidirectional Traffic\n",
    "- Busy corridor, people going both ways\n",
    "- Line in middle of corridor\n",
    "- Count both directions\n",
    "- Analyze traffic patterns\n",
    "\n",
    "==================================================\n",
    "\n",
    "HANDLING EDGE CASES:\n",
    "\n",
    "Case 1: Person Lingers on Line\n",
    "- Problem: Multiple crossings detected\n",
    "- Solution: Cooldown period per track\n",
    "- Only count once per track per X seconds\n",
    "\n",
    "Case 2: Person Crosses Back and Forth\n",
    "- Problem: Inflated count\n",
    "- Solution: Track last crossing direction\n",
    "- Only count if direction changes\n",
    "\n",
    "Case 3: Multiple People Cross Simultaneously\n",
    "- Problem: Accurate counting\n",
    "- Solution: Track each person's ID separately\n",
    "- Count each unique crossing\n",
    "\n",
    "Case 4: Person Partially Crosses\n",
    "- Problem: False positive\n",
    "- Solution: Require full crossing\n",
    "- Check both feet/bottom of bbox crossed\n",
    "\n",
    "==================================================\n",
    "\n",
    "PERFORMANCE CONSIDERATIONS:\n",
    "\n",
    "Accuracy Factors:\n",
    "- Tracking quality (ID consistency)\n",
    "- Line placement (optimal location)\n",
    "- Camera angle (perpendicular is best)\n",
    "- Lighting conditions\n",
    "- Occlusions at line\n",
    "\n",
    "Expected Accuracy:\n",
    "- Optimal conditions: 95-98%\n",
    "- Normal conditions: 90-95%\n",
    "- Challenging conditions: 85-90%\n",
    "\n",
    "Common Errors:\n",
    "- Missed counts: Track lost at line\n",
    "- Double counts: ID switch at line\n",
    "- Wrong direction: Tracking jitter near line\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "üìê VISUAL REPRESENTATION:\n",
    "\n",
    "Line-Crossing Detection:\n",
    "\n",
    "Frame N-1:                Frame N:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ\n",
    "‚îÇ    ‚óè        ‚îÇ          ‚îÇ             ‚îÇ\n",
    "‚îÇ   P1        ‚îÇ          ‚îÇ      ‚óè      ‚îÇ\n",
    "‚îÇ             ‚îÇ          ‚îÇ     P2      ‚îÇ\n",
    "‚îÇ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ LINE     ‚îÇ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ LINE\n",
    "‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ\n",
    "‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "P1 above line (cross < 0)    P2 below line (cross > 0)\n",
    "                    ‚Üí CROSSING DETECTED! ‚úì\n",
    "\n",
    "Direction Determination:\n",
    "- P1 to P2: Negative to Positive ‚Üí Direction: DOWN (or IN)\n",
    "- P2 to P1: Positive to Negative ‚Üí Direction: UP (or OUT)\n",
    "\n",
    "==================================================\n",
    "\n",
    "IMPLEMENTATION STEPS:\n",
    "\n",
    "1. Define line coordinates\n",
    "   line_start = (x1, y1)\n",
    "   line_end = (x2, y2)\n",
    "\n",
    "2. Track object centers\n",
    "   track_positions[track_id] = [(x, y), ...]\n",
    "\n",
    "3. Calculate cross product\n",
    "   cross_prev = calculate_cross_product(prev_pos, line)\n",
    "   cross_curr = calculate_cross_product(curr_pos, line)\n",
    "\n",
    "4. Detect crossing\n",
    "   if sign(cross_prev) != sign(cross_curr):\n",
    "       crossing_detected = True\n",
    "\n",
    "5. Determine direction\n",
    "   if cross_prev < 0 and cross_curr > 0:\n",
    "       direction = \"ENTERING\"\n",
    "   elif cross_prev > 0 and cross_curr < 0:\n",
    "       direction = \"EXITING\"\n",
    "\n",
    "6. Update counters\n",
    "   if direction == \"ENTERING\":\n",
    "       count_in += 1\n",
    "   elif direction == \"EXITING\":\n",
    "       count_out += 1\n",
    "   \n",
    "   current_occupancy = count_in - count_out\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 2.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db932be-8b44-4893-9890-b87a161a74c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 2.2: Implement Line-Crossing Detection\n",
      "================================================================================\n",
      "‚úÖ Class created: PeopleCounter\n",
      "\n",
      "üìä Features:\n",
      "   ‚Ä¢ Line-crossing detection (cross product method)\n",
      "   ‚Ä¢ Direction-aware counting (IN vs OUT)\n",
      "   ‚Ä¢ Duplicate prevention (track last crossing)\n",
      "   ‚Ä¢ Oscillation handling (direction change required)\n",
      "   ‚Ä¢ Current occupancy calculation (IN - OUT)\n",
      "   ‚Ä¢ Visualization (line + counters)\n",
      "\n",
      "üß™ Testing PeopleCounter class...\n",
      "‚úÖ PeopleCounter initialized\n",
      "   Line: (100, 360) ‚Üí (1180, 360)\n",
      "\n",
      "‚úÖ Test counter created!\n",
      "   Initial counts: {'in': 0, 'out': 0, 'current': 0, 'total': 0}\n",
      "\n",
      "‚úÖ Exercise 2.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 2.2: IMPLEMENT LINE-CROSSING DETECTION\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 2.2: Implement Line-Crossing Detection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Implementation Details\n",
    "\n",
    "We'll implement:\n",
    "1. Line definition (interactive or fixed)\n",
    "2. Cross product calculation\n",
    "3. Crossing detection logic\n",
    "4. Direction determination\n",
    "5. Count tracking\n",
    "6. Visualization\n",
    "\n",
    "Data Structures:\n",
    "- line_coords: (x1, y1, x2, y2)\n",
    "- track_history: {track_id: [(x, y), ...]}\n",
    "- crossed_tracks: {track_id: last_direction}\n",
    "- counters: {in: 0, out: 0, current: 0}\n",
    "\"\"\"\n",
    "\n",
    "class PeopleCounter:\n",
    "    \"\"\"\n",
    "    People counting system using line-crossing detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, line_start, line_end):\n",
    "        \"\"\"\n",
    "        Initialize counter with line coordinates\n",
    "        \n",
    "        Args:\n",
    "            line_start: (x, y) tuple for line start point\n",
    "            line_end: (x, y) tuple for line end point\n",
    "        \"\"\"\n",
    "        self.line_start = np.array(line_start)\n",
    "        self.line_end = np.array(line_end)\n",
    "        \n",
    "        # Track positions history (last 2 positions per track)\n",
    "        self.track_positions = defaultdict(lambda: deque(maxlen=2))\n",
    "        \n",
    "        # Tracks that have crossed (to prevent double counting)\n",
    "        self.crossed_tracks = {}\n",
    "        \n",
    "        # Counters\n",
    "        self.count_in = 0\n",
    "        self.count_out = 0\n",
    "        self.total_crossings = 0\n",
    "        \n",
    "        print(f\"‚úÖ PeopleCounter initialized\")\n",
    "        print(f\"   Line: {line_start} ‚Üí {line_end}\")\n",
    "    \n",
    "    def _calculate_cross_product(self, point):\n",
    "        \"\"\"\n",
    "        Calculate cross product to determine which side of line point is on\n",
    "        \n",
    "        Args:\n",
    "            point: (x, y) tuple\n",
    "            \n",
    "        Returns:\n",
    "            float: cross product (sign indicates side)\n",
    "        \"\"\"\n",
    "        # Line vector\n",
    "        line_vec = self.line_end - self.line_start\n",
    "        \n",
    "        # Point vector (from line start to point)\n",
    "        point_vec = np.array(point) - self.line_start\n",
    "        \n",
    "        # Cross product (2D)\n",
    "        cross = line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0]\n",
    "        \n",
    "        return cross\n",
    "    \n",
    "    def update(self, tracks):\n",
    "        \"\"\"\n",
    "        Update counter with new tracks\n",
    "        \n",
    "        Args:\n",
    "            tracks: List of tracks from DeepSORT\n",
    "            \n",
    "        Returns:\n",
    "            dict: Crossing events {track_id: direction}\n",
    "        \"\"\"\n",
    "        crossings = {}\n",
    "        \n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            \n",
    "            track_id = track.track_id\n",
    "            bbox = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            \n",
    "            # Get center point (bottom-center for better accuracy)\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int(y2)  # Bottom of bounding box\n",
    "            center = (center_x, center_y)\n",
    "            \n",
    "            # Add to position history\n",
    "            self.track_positions[track_id].append(center)\n",
    "            \n",
    "            # Need at least 2 positions to detect crossing\n",
    "            if len(self.track_positions[track_id]) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get previous and current positions\n",
    "            prev_pos = self.track_positions[track_id][0]\n",
    "            curr_pos = self.track_positions[track_id][1]\n",
    "            \n",
    "            # Calculate cross products\n",
    "            cross_prev = self._calculate_cross_product(prev_pos)\n",
    "            cross_curr = self._calculate_cross_product(curr_pos)\n",
    "            \n",
    "            # Detect crossing (sign change)\n",
    "            if np.sign(cross_prev) != np.sign(cross_curr) and cross_prev != 0:\n",
    "                \n",
    "                # Check if already counted this track recently\n",
    "                if track_id in self.crossed_tracks:\n",
    "                    last_direction = self.crossed_tracks[track_id]\n",
    "                    \n",
    "                    # Determine current direction\n",
    "                    if cross_prev < 0 and cross_curr > 0:\n",
    "                        curr_direction = \"IN\"\n",
    "                    else:\n",
    "                        curr_direction = \"OUT\"\n",
    "                    \n",
    "                    # Only count if direction changed (prevents oscillation)\n",
    "                    if curr_direction == last_direction:\n",
    "                        continue\n",
    "                \n",
    "                # Determine direction\n",
    "                if cross_prev < 0 and cross_curr > 0:\n",
    "                    direction = \"IN\"\n",
    "                    self.count_in += 1\n",
    "                else:\n",
    "                    direction = \"OUT\"\n",
    "                    self.count_out += 1\n",
    "                \n",
    "                self.total_crossings += 1\n",
    "                self.crossed_tracks[track_id] = direction\n",
    "                crossings[track_id] = direction\n",
    "        \n",
    "        return crossings\n",
    "    \n",
    "    def get_counts(self):\n",
    "        \"\"\"Get current counts\"\"\"\n",
    "        return {\n",
    "            'in': self.count_in,\n",
    "            'out': self.count_out,\n",
    "            'current': self.count_in - self.count_out,\n",
    "            'total': self.total_crossings\n",
    "        }\n",
    "    \n",
    "    def draw_line(self, frame):\n",
    "        \"\"\"\n",
    "        Draw counting line on frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame\n",
    "        \"\"\"\n",
    "        annotated = frame.copy()\n",
    "        \n",
    "        # Draw line\n",
    "        cv2.line(annotated, tuple(self.line_start.astype(int)), \n",
    "                tuple(self.line_end.astype(int)), (0, 255, 255), 3)\n",
    "        \n",
    "        # Draw endpoints\n",
    "        cv2.circle(annotated, tuple(self.line_start.astype(int)), 8, (0, 255, 255), -1)\n",
    "        cv2.circle(annotated, tuple(self.line_end.astype(int)), 8, (0, 255, 255), -1)\n",
    "        \n",
    "        # Add label\n",
    "        mid_point = ((self.line_start + self.line_end) / 2).astype(int)\n",
    "        cv2.putText(annotated, \"COUNTING LINE\", tuple(mid_point - [0, 15]),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        \n",
    "        return annotated\n",
    "    \n",
    "    def draw_counts(self, frame):\n",
    "        \"\"\"\n",
    "        Draw count information on frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame\n",
    "        \"\"\"\n",
    "        annotated = frame.copy()\n",
    "        counts = self.get_counts()\n",
    "        \n",
    "        # Semi-transparent panel\n",
    "        overlay = annotated.copy()\n",
    "        cv2.rectangle(overlay, (10, 200), (350, 400), (0, 0, 0), -1)\n",
    "        cv2.addWeighted(overlay, 0.6, annotated, 0.4, 0, annotated)\n",
    "        \n",
    "        # Draw counts\n",
    "        y_offset = 240\n",
    "        cv2.putText(annotated, \"PEOPLE COUNT\", (20, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        y_offset += 40\n",
    "        cv2.putText(annotated, f\"IN:  {counts['in']}\", (20, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        y_offset += 40\n",
    "        cv2.putText(annotated, f\"OUT: {counts['out']}\", (20, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        \n",
    "        y_offset += 40\n",
    "        cv2.putText(annotated, f\"Current: {counts['current']}\", (20, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "        \n",
    "        return annotated\n",
    "\n",
    "print(\"‚úÖ Class created: PeopleCounter\")\n",
    "print(\"\\nüìä Features:\")\n",
    "print(\"   ‚Ä¢ Line-crossing detection (cross product method)\")\n",
    "print(\"   ‚Ä¢ Direction-aware counting (IN vs OUT)\")\n",
    "print(\"   ‚Ä¢ Duplicate prevention (track last crossing)\")\n",
    "print(\"   ‚Ä¢ Oscillation handling (direction change required)\")\n",
    "print(\"   ‚Ä¢ Current occupancy calculation (IN - OUT)\")\n",
    "print(\"   ‚Ä¢ Visualization (line + counters)\")\n",
    "\n",
    "print(\"\\nüß™ Testing PeopleCounter class...\")\n",
    "\n",
    "# Create test counter (horizontal line in middle of 720p frame)\n",
    "test_counter = PeopleCounter(\n",
    "    line_start=(100, 360),  # Left side, middle height\n",
    "    line_end=(1180, 360)    # Right side, middle height\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Test counter created!\")\n",
    "print(f\"   Initial counts: {test_counter.get_counts()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 2.2 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fbe2b9b-777c-4f75-8fdc-60e44ad1843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 2.3: People Counting Demo with Webcam\n",
      "================================================================================\n",
      "\n",
      "üé• COMPLETE PEOPLE COUNTING DEMO\n",
      "\n",
      "Below is the full code for webcam people counting.\n",
      "This integrates YOLO + DeepSORT + PeopleCounter!\n",
      "\n",
      "Features:\n",
      "‚úì Real-time detection + tracking\n",
      "‚úì Line-crossing detection\n",
      "‚úì Direction-aware counting (IN/OUT)\n",
      "‚úì Current occupancy tracking\n",
      "‚úì Visual counting line\n",
      "‚úì Counter display panel\n",
      "‚úì FPS monitoring\n",
      "\n",
      "üìù To run this demo:\n",
      "1. Copy the code below to a new cell\n",
      "2. Execute the cell\n",
      "3. Your webcam will open\n",
      "4. Walk across the yellow line to test counting!\n",
      "5. Try going both directions (IN and OUT)\n",
      "6. Press 'q' to quit\n",
      "\n",
      "‚ö†Ô∏è  Note: Adjust line position based on your camera view\n",
      "         Default line is horizontal in middle of frame\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CODE: PEOPLE COUNTING DEMO\n",
      "================================================================================\n",
      "\n",
      "Copy this code to a new cell to run people counting:\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "import cv2\n",
      "import numpy as np\n",
      "from ultralytics import YOLO\n",
      "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
      "import time\n",
      "\n",
      "# Load models\n",
      "print(\"Loading models...\")\n",
      "model = YOLO('yolov8n.pt')\n",
      "tracker = DeepSort(max_age=30, n_init=3, embedder=\"mobilenet\", embedder_gpu=False)\n",
      "\n",
      "# Initialize counter (adjust line position for your camera!)\n",
      "# Line coordinates: (x1, y1) to (x2, y2)\n",
      "# Default: horizontal line in middle of 640x480 frame\n",
      "counter = PeopleCounter(\n",
      "    line_start=(50, 240),   # Left side, middle height\n",
      "    line_end=(590, 240)     # Right side, middle height\n",
      ")\n",
      "\n",
      "# Open webcam\n",
      "cap = cv2.VideoCapture(0)\n",
      "\n",
      "if not cap.isOpened():\n",
      "    print(\"‚ùå Cannot open webcam!\")\n",
      "else:\n",
      "    print(\"‚úÖ Webcam opened!\")\n",
      "    print(\"üé• Starting people counting... (Press 'q' to quit)\")\n",
      "    print(\"üí° Walk across the YELLOW line to test counting!\")\n",
      "\n",
      "    # FPS tracking\n",
      "    fps_counter = 0\n",
      "    start_time = time.time()\n",
      "    fps = 0\n",
      "\n",
      "    while True:\n",
      "        ret, frame = cap.read()\n",
      "        if not ret:\n",
      "            break\n",
      "\n",
      "        # 1. YOLO Detection\n",
      "        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)\n",
      "        detections = results[0].boxes\n",
      "\n",
      "        # 2. Convert to DeepSORT format\n",
      "        deepsort_input = []\n",
      "        for box in detections:\n",
      "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
      "            conf = float(box.conf[0])\n",
      "            w = x2 - x1\n",
      "            h = y2 - y1\n",
      "            deepsort_input.append(([x1, y1, w, h], conf, 'person'))\n",
      "\n",
      "        # 3. Update tracker\n",
      "        tracks = tracker.update_tracks(deepsort_input, frame=frame)\n",
      "\n",
      "        # 4. Update counter (detect crossings)\n",
      "        crossings = counter.update(tracks)\n",
      "\n",
      "        # 5. Visualize tracks\n",
      "        for track in tracks:\n",
      "            if not track.is_confirmed():\n",
      "                continue\n",
      "\n",
      "            track_id = track.track_id\n",
      "            bbox = track.to_ltrb()\n",
      "            x1, y1, x2, y2 = map(int, bbox)\n",
      "\n",
      "            # Color: Green normally, Red if just crossed\n",
      "            if track_id in crossings:\n",
      "                color = (0, 0, 255)  # Red\n",
      "                direction = crossings[track_id]\n",
      "                label = f'ID: {track_id} ({direction})'\n",
      "            else:\n",
      "                color = (0, 255, 0)  # Green\n",
      "                label = f'ID: {track_id}'\n",
      "\n",
      "            # Draw bounding box\n",
      "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
      "\n",
      "            # Draw ID\n",
      "            cv2.putText(frame, label, (x1, y1 - 10),\n",
      "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
      "\n",
      "        # 6. Draw counting line\n",
      "        frame = counter.draw_line(frame)\n",
      "\n",
      "        # 7. Draw counter panel\n",
      "        frame = counter.draw_counts(frame)\n",
      "\n",
      "        # 8. Calculate FPS\n",
      "        fps_counter += 1\n",
      "        if fps_counter % 30 == 0:\n",
      "            elapsed = time.time() - start_time\n",
      "            fps = 30 / elapsed\n",
      "            start_time = time.time()\n",
      "\n",
      "        # 9. Display FPS\n",
      "        cv2.putText(frame, f'FPS: {fps:.1f}', (10, 30),\n",
      "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
      "\n",
      "        # 10. Show frame\n",
      "        cv2.imshow('People Counting System', frame)\n",
      "\n",
      "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "            break\n",
      "\n",
      "    cap.release()\n",
      "    cv2.destroyAllWindows()\n",
      "\n",
      "    # Final statistics\n",
      "    final_counts = counter.get_counts()\n",
      "    print(f\"\\nüìä Final Statistics:\")\n",
      "    print(f\"   ‚Ä¢ People IN: {final_counts['in']}\")\n",
      "    print(f\"   ‚Ä¢ People OUT: {final_counts['out']}\")\n",
      "    print(f\"   ‚Ä¢ Current occupancy: {final_counts['current']}\")\n",
      "    print(f\"   ‚Ä¢ Total crossings: {final_counts['total']}\")\n",
      "    print(f\"   ‚Ä¢ Average FPS: {fps:.1f}\")\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üí° What to Observe:\n",
      "   ‚Ä¢ Yellow horizontal line in middle of frame\n",
      "   ‚Ä¢ Each person gets unique green box + ID\n",
      "   ‚Ä¢ Box turns RED when crossing line\n",
      "   ‚Ä¢ Counter panel shows IN, OUT, Current\n",
      "   ‚Ä¢ IN increases when crossing one direction\n",
      "   ‚Ä¢ OUT increases when crossing other direction\n",
      "   ‚Ä¢ Current = IN - OUT (occupancy)\n",
      "\n",
      "üß™ Test Scenarios:\n",
      "   1. Walk across line left-to-right (should count IN)\n",
      "   2. Walk across line right-to-left (should count OUT)\n",
      "   3. Walk back and forth (alternates IN/OUT)\n",
      "   4. Multiple people cross together (each counted)\n",
      "   5. Person lingers on line (counted only once)\n",
      "\n",
      "‚úÖ Exercise 2.3 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 2.3: PEOPLE COUNTING DEMO\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 2.3: People Counting Demo with Webcam\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Complete Counting System\n",
    "\n",
    "Integration Steps:\n",
    "1. Initialize YOLO detector\n",
    "2. Initialize DeepSORT tracker\n",
    "3. Initialize PeopleCounter\n",
    "4. For each frame:\n",
    "   a. Detect with YOLO\n",
    "   b. Track with DeepSORT\n",
    "   c. Update counter\n",
    "   d. Visualize everything\n",
    "5. Display results\n",
    "\n",
    "What to Observe:\n",
    "- People get unique IDs\n",
    "- IDs persist as they move\n",
    "- Counter increments when crossing line\n",
    "- Direction is detected (IN vs OUT)\n",
    "- Current occupancy updates\n",
    "- Visualization clear and informative\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "üé• COMPLETE PEOPLE COUNTING DEMO\n",
    "\n",
    "Below is the full code for webcam people counting.\n",
    "This integrates YOLO + DeepSORT + PeopleCounter!\n",
    "\n",
    "Features:\n",
    "‚úì Real-time detection + tracking\n",
    "‚úì Line-crossing detection\n",
    "‚úì Direction-aware counting (IN/OUT)\n",
    "‚úì Current occupancy tracking\n",
    "‚úì Visual counting line\n",
    "‚úì Counter display panel\n",
    "‚úì FPS monitoring\n",
    "\n",
    "üìù To run this demo:\n",
    "1. Copy the code below to a new cell\n",
    "2. Execute the cell\n",
    "3. Your webcam will open\n",
    "4. Walk across the yellow line to test counting!\n",
    "5. Try going both directions (IN and OUT)\n",
    "6. Press 'q' to quit\n",
    "\n",
    "‚ö†Ô∏è  Note: Adjust line position based on your camera view\n",
    "         Default line is horizontal in middle of frame\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CODE: PEOPLE COUNTING DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Copy this code to a new cell to run people counting:\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "model = YOLO('yolov8n.pt')\n",
    "tracker = DeepSort(max_age=30, n_init=3, embedder=\"mobilenet\", embedder_gpu=False)\n",
    "\n",
    "# Initialize counter (adjust line position for your camera!)\n",
    "# Line coordinates: (x1, y1) to (x2, y2)\n",
    "# Default: horizontal line in middle of 640x480 frame\n",
    "counter = PeopleCounter(\n",
    "    line_start=(50, 240),   # Left side, middle height\n",
    "    line_end=(590, 240)     # Right side, middle height\n",
    ")\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Cannot open webcam!\")\n",
    "else:\n",
    "    print(\"‚úÖ Webcam opened!\")\n",
    "    print(\"üé• Starting people counting... (Press 'q' to quit)\")\n",
    "    print(\"üí° Walk across the YELLOW line to test counting!\")\n",
    "    \n",
    "    # FPS tracking\n",
    "    fps_counter = 0\n",
    "    start_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 1. YOLO Detection\n",
    "        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)\n",
    "        detections = results[0].boxes\n",
    "        \n",
    "        # 2. Convert to DeepSORT format\n",
    "        deepsort_input = []\n",
    "        for box in detections:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = float(box.conf[0])\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            deepsort_input.append(([x1, y1, w, h], conf, 'person'))\n",
    "        \n",
    "        # 3. Update tracker\n",
    "        tracks = tracker.update_tracks(deepsort_input, frame=frame)\n",
    "        \n",
    "        # 4. Update counter (detect crossings)\n",
    "        crossings = counter.update(tracks)\n",
    "        \n",
    "        # 5. Visualize tracks\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            \n",
    "            track_id = track.track_id\n",
    "            bbox = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            \n",
    "            # Color: Green normally, Red if just crossed\n",
    "            if track_id in crossings:\n",
    "                color = (0, 0, 255)  # Red\n",
    "                direction = crossings[track_id]\n",
    "                label = f'ID: {track_id} ({direction})'\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green\n",
    "                label = f'ID: {track_id}'\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Draw ID\n",
    "            cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        # 6. Draw counting line\n",
    "        frame = counter.draw_line(frame)\n",
    "        \n",
    "        # 7. Draw counter panel\n",
    "        frame = counter.draw_counts(frame)\n",
    "        \n",
    "        # 8. Calculate FPS\n",
    "        fps_counter += 1\n",
    "        if fps_counter % 30 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            fps = 30 / elapsed\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # 9. Display FPS\n",
    "        cv2.putText(frame, f'FPS: {fps:.1f}', (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        # 10. Show frame\n",
    "        cv2.imshow('People Counting System', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Final statistics\n",
    "    final_counts = counter.get_counts()\n",
    "    print(f\"\\\\nüìä Final Statistics:\")\n",
    "    print(f\"   ‚Ä¢ People IN: {final_counts['in']}\")\n",
    "    print(f\"   ‚Ä¢ People OUT: {final_counts['out']}\")\n",
    "    print(f\"   ‚Ä¢ Current occupancy: {final_counts['current']}\")\n",
    "    print(f\"   ‚Ä¢ Total crossings: {final_counts['total']}\")\n",
    "    print(f\"   ‚Ä¢ Average FPS: {fps:.1f}\")\n",
    "-----------------------------------------------------------------------\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° What to Observe:\")\n",
    "print(\"   ‚Ä¢ Yellow horizontal line in middle of frame\")\n",
    "print(\"   ‚Ä¢ Each person gets unique green box + ID\")\n",
    "print(\"   ‚Ä¢ Box turns RED when crossing line\")\n",
    "print(\"   ‚Ä¢ Counter panel shows IN, OUT, Current\")\n",
    "print(\"   ‚Ä¢ IN increases when crossing one direction\")\n",
    "print(\"   ‚Ä¢ OUT increases when crossing other direction\")\n",
    "print(\"   ‚Ä¢ Current = IN - OUT (occupancy)\")\n",
    "\n",
    "print(\"\\nüß™ Test Scenarios:\")\n",
    "print(\"   1. Walk across line left-to-right (should count IN)\")\n",
    "print(\"   2. Walk across line right-to-left (should count OUT)\")\n",
    "print(\"   3. Walk back and forth (alternates IN/OUT)\")\n",
    "print(\"   4. Multiple people cross together (each counted)\")\n",
    "print(\"   5. Person lingers on line (counted only once)\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 2.3 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd41da54-6ff6-4c93-8979-3421bca8d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üè¢ PART 3: ZONE-BASED MONITORING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üè¢ PART 3: ZONE-BASED MONITORING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95add0bf-d45e-419c-a2c6-b7685316acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 3.1: Understanding Zone-Based Monitoring\n",
      "================================================================================\n",
      "\n",
      "üìä ZONE MONITORING EXAMPLE:\n",
      "\n",
      "Scenario: Office Building Floor\n",
      "\n",
      "Zones Defined:\n",
      "1. Lobby (public, unlimited capacity)\n",
      "2. Meeting Room A (capacity: 10)\n",
      "3. Meeting Room B (capacity: 6)\n",
      "4. Server Room (restricted, capacity: 2)\n",
      "5. Executive Area (restricted, capacity: 5)\n",
      "\n",
      "Tracking in Action:\n",
      "- Person ID 1 enters Lobby ‚Üí No alert\n",
      "- Person ID 1 enters Meeting Room A ‚Üí Entry logged\n",
      "- 11 people in Meeting Room A ‚Üí CAPACITY ALERT\n",
      "- Person ID 5 enters Server Room ‚Üí RESTRICTED ALERT (if unauthorized)\n",
      "- Person ID 3 in Server Room for 45 min ‚Üí DWELL TIME ALERT\n",
      "\n",
      "System Response:\n",
      "- Real-time alerts displayed\n",
      "- Email sent to security\n",
      "- Log entry created\n",
      "- Dashboard updated\n",
      "\n",
      "Benefits:\n",
      "‚úì Automated capacity management\n",
      "‚úì Security breach detection\n",
      "‚úì Occupancy analytics\n",
      "‚úì Safety compliance\n",
      "‚úì Resource optimization\n",
      "\n",
      "\n",
      "‚úÖ Exercise 3.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 3.1: UNDERSTAND ZONE-BASED MONITORING\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 3.1: Understanding Zone-Based Monitoring\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Zone-Based Tracking and Monitoring\n",
    "\n",
    "What is Zone Monitoring?\n",
    "- Define specific areas (zones) in camera view\n",
    "- Track which people are in which zones\n",
    "- Count occupancy per zone\n",
    "- Detect unauthorized access\n",
    "- Alert on capacity violations\n",
    "\n",
    "Why Zone Monitoring?\n",
    "‚úì Restricted area access control\n",
    "‚úì Capacity management per area\n",
    "‚úì Safety zone monitoring (hazard areas)\n",
    "‚úì Heat map generation (where people spend time)\n",
    "‚úì Traffic flow analysis\n",
    "\n",
    "==================================================\n",
    "\n",
    "ZONE DEFINITION METHODS:\n",
    "\n",
    "Method 1: Rectangular Zones\n",
    "- Simplest approach\n",
    "- Define by (x1, y1, x2, y2) corners\n",
    "- Check if point inside rectangle\n",
    "- Fast computation\n",
    "- Limited flexibility\n",
    "\n",
    "Method 2: Polygonal Zones\n",
    "- More flexible shapes\n",
    "- Define by list of vertices [(x1,y1), (x2,y2), ...]\n",
    "- Point-in-polygon algorithm\n",
    "- Can match room layouts\n",
    "- Slightly slower\n",
    "\n",
    "Method 3: Multiple Zones\n",
    "- Divide space into regions\n",
    "- Each zone has properties (name, capacity, restricted)\n",
    "- Track which people in which zones\n",
    "- Enable complex monitoring rules\n",
    "\n",
    "==================================================\n",
    "\n",
    "POINT-IN-POLYGON ALGORITHM:\n",
    "\n",
    "Ray Casting Method:\n",
    "1. Draw ray from point to infinity (horizontal)\n",
    "2. Count intersections with polygon edges\n",
    "3. Odd count = inside polygon\n",
    "4. Even count = outside polygon\n",
    "\n",
    "OpenCV Implementation:\n",
    "- cv2.pointPolygonTest(contour, point, measureDist)\n",
    "- Returns: >0 inside, =0 on edge, <0 outside\n",
    "- Fast and reliable\n",
    "- Built-in to OpenCV\n",
    "\n",
    "==================================================\n",
    "\n",
    "ZONE MONITORING USE CASES:\n",
    "\n",
    "Security System:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Public Area                        ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ\n",
    "‚îÇ  ‚îÇ Restricted‚îÇ                      ‚îÇ\n",
    "‚îÇ  ‚îÇ   Zone    ‚îÇ  ‚Üê Authorized only   ‚îÇ\n",
    "‚îÇ  ‚îÇ  (Admin)  ‚îÇ                      ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
    "‚îÇ  ‚îÇ  Meeting Room   ‚îÇ ‚Üê Max 10 people‚îÇ\n",
    "‚îÇ  ‚îÇ  (Capacity: 10) ‚îÇ                ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Factory Safety:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ    ‚ö†Ô∏è  Hazard Zone ‚ö†Ô∏è               ‚îÇ\n",
    "‚îÇ    (Require PPE)                    ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ\n",
    "‚îÇ    Safe Work Area                  ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Retail Store:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
    "‚îÇ  ‚îÇDept1‚îÇ  ‚îÇDept2‚îÇ  ‚îÇDept3‚îÇ        ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ         Checkout Area               ‚îÇ\n",
    "‚îÇ       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "- Track dwell time per department\n",
    "- Queue length at checkout\n",
    "- Traffic patterns\n",
    "\n",
    "==================================================\n",
    "\n",
    "ZONE PROPERTIES:\n",
    "\n",
    "Basic Properties:\n",
    "- name: \"Meeting Room A\"\n",
    "- vertices: [(x1,y1), (x2,y2), ...]\n",
    "- capacity: 10 (max people)\n",
    "- restricted: True/False\n",
    "\n",
    "Monitoring Properties:\n",
    "- current_count: 3 (people currently in zone)\n",
    "- total_entries: 47 (cumulative)\n",
    "- total_exits: 44\n",
    "- avg_dwell_time: 5.2 minutes\n",
    "\n",
    "Alert Triggers:\n",
    "- Over capacity: current_count > capacity\n",
    "- Unauthorized: person_id not in authorized_list\n",
    "- Too long: dwell_time > max_allowed\n",
    "- Safety: no_ppe_detected in hazard_zone\n",
    "\n",
    "==================================================\n",
    "\n",
    "IMPLEMENTATION APPROACH:\n",
    "\n",
    "Data Structures:\n",
    "\n",
    "Zone Class:\n",
    "class Zone:\n",
    "    - name\n",
    "    - vertices (polygon points)\n",
    "    - capacity\n",
    "    - restricted\n",
    "    - current_occupants (set of track IDs)\n",
    "    - entry_times (dict: track_id -> timestamp)\n",
    "    - total_entries\n",
    "    - total_exits\n",
    "\n",
    "Methods:\n",
    "- contains_point(point) ‚Üí bool\n",
    "- update(tracks) ‚Üí events dict\n",
    "- get_occupancy() ‚Üí int\n",
    "- is_over_capacity() ‚Üí bool\n",
    "- get_dwell_times() ‚Üí dict\n",
    "- draw(frame) ‚Üí annotated frame\n",
    "\n",
    "==================================================\n",
    "\n",
    "ALGORITHM FLOW:\n",
    "\n",
    "For each frame:\n",
    "1. Get tracked objects from DeepSORT\n",
    "2. For each track:\n",
    "   a. Get object center point (x, y)\n",
    "   b. For each zone:\n",
    "      - Check if point inside zone\n",
    "      - If inside:\n",
    "        * Add to current_occupants\n",
    "        * If new: record entry time, increment entries\n",
    "      - If not inside but was before:\n",
    "        * Remove from current_occupants\n",
    "        * Delete entry time, increment exits\n",
    "3. Check violations:\n",
    "   - Occupancy > capacity?\n",
    "   - Track in restricted zone?\n",
    "   - Dwell time too long?\n",
    "4. Generate alerts if violations detected\n",
    "5. Visualize zones on frame\n",
    "\n",
    "==================================================\n",
    "\n",
    "VISUALIZATION TECHNIQUES:\n",
    "\n",
    "1. Semi-transparent overlays\n",
    "   ‚Ä¢ cv2.fillPoly() with transparency\n",
    "   ‚Ä¢ Different colors per zone\n",
    "   ‚Ä¢ Doesn't obscure video\n",
    "\n",
    "2. Zone labels\n",
    "   ‚Ä¢ Name at centroid\n",
    "   ‚Ä¢ Current occupancy count\n",
    "   ‚Ä¢ Capacity status (color-coded)\n",
    "\n",
    "3. Track indicators\n",
    "   ‚Ä¢ Highlight tracks inside zones\n",
    "   ‚Ä¢ Show which zone each person is in\n",
    "   ‚Ä¢ Entry/exit animations\n",
    "\n",
    "4. Alert overlays\n",
    "   ‚Ä¢ Flashing border for violations\n",
    "   ‚Ä¢ Text alerts on screen\n",
    "   ‚Ä¢ Color changes (red for violations)\n",
    "\n",
    "==================================================\n",
    "\n",
    "PERFORMANCE CONSIDERATIONS:\n",
    "\n",
    "Speed:\n",
    "- Point-in-polygon: O(n) where n = vertices\n",
    "- Multiple zones: O(m √ó t) where m = zones, t = tracks\n",
    "- Negligible overhead for <10 zones\n",
    "\n",
    "Accuracy:\n",
    "- Use bottom-center of bbox (person's feet)\n",
    "- More accurate than bbox center\n",
    "- Handles partially inside zones\n",
    "\n",
    "Memory:\n",
    "- Track history per zone\n",
    "- Limit stored history (e.g., last 100 entries)\n",
    "- Periodic cleanup of old data\n",
    "\n",
    "==================================================\n",
    "\n",
    "ADVANCED FEATURES:\n",
    "\n",
    "1. Zone Hierarchies\n",
    "   ‚Ä¢ Nested zones (room inside building)\n",
    "   ‚Ä¢ Parent-child relationships\n",
    "   ‚Ä¢ Inheritance of properties\n",
    "\n",
    "2. Time-based Rules\n",
    "   ‚Ä¢ Zone restrictions by time of day\n",
    "   ‚Ä¢ Capacity limits vary by schedule\n",
    "   ‚Ä¢ Automated alerts during specific hours\n",
    "\n",
    "3. Heat Maps\n",
    "   ‚Ä¢ Accumulate occupancy over time\n",
    "   ‚Ä¢ Visualize popular areas\n",
    "   ‚Ä¢ Optimize layout based on traffic\n",
    "\n",
    "4. Path Analysis\n",
    "   ‚Ä¢ Common entry/exit paths\n",
    "   ‚Ä¢ Time to traverse zones\n",
    "   ‚Ä¢ Bottleneck identification\n",
    "\n",
    "5. Multi-camera Zones\n",
    "   ‚Ä¢ Zones span multiple cameras\n",
    "   ‚Ä¢ Track handoff between cameras\n",
    "   ‚Ä¢ Global zone monitoring\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "üìä ZONE MONITORING EXAMPLE:\n",
    "\n",
    "Scenario: Office Building Floor\n",
    "\n",
    "Zones Defined:\n",
    "1. Lobby (public, unlimited capacity)\n",
    "2. Meeting Room A (capacity: 10)\n",
    "3. Meeting Room B (capacity: 6)\n",
    "4. Server Room (restricted, capacity: 2)\n",
    "5. Executive Area (restricted, capacity: 5)\n",
    "\n",
    "Tracking in Action:\n",
    "- Person ID 1 enters Lobby ‚Üí No alert\n",
    "- Person ID 1 enters Meeting Room A ‚Üí Entry logged\n",
    "- 11 people in Meeting Room A ‚Üí CAPACITY ALERT\n",
    "- Person ID 5 enters Server Room ‚Üí RESTRICTED ALERT (if unauthorized)\n",
    "- Person ID 3 in Server Room for 45 min ‚Üí DWELL TIME ALERT\n",
    "\n",
    "System Response:\n",
    "- Real-time alerts displayed\n",
    "- Email sent to security\n",
    "- Log entry created\n",
    "- Dashboard updated\n",
    "\n",
    "Benefits:\n",
    "‚úì Automated capacity management\n",
    "‚úì Security breach detection\n",
    "‚úì Occupancy analytics\n",
    "‚úì Safety compliance\n",
    "‚úì Resource optimization\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 3.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12a53f4-d58d-42f5-9b66-35ae97cccb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 3.2: Implement Zone Monitoring System\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Class created: Zone\n",
      "\n",
      "üìä Features:\n",
      "   ‚Ä¢ Polygon-based zone definition\n",
      "   ‚Ä¢ Point-in-polygon detection (cv2)\n",
      "   ‚Ä¢ Occupancy tracking with entry/exit events\n",
      "   ‚Ä¢ Capacity monitoring and alerts\n",
      "   ‚Ä¢ Dwell time calculation\n",
      "   ‚Ä¢ Visual overlay with transparency\n",
      "\n",
      "‚úÖ Exercise 3.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 3.2: IMPLEMENT ZONE MONITORING\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 3.2: Implement Zone Monitoring System\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Zone Monitoring Implementation\n",
    "\n",
    "Data Structures Needed:\n",
    "1. Zone definition (polygon vertices)\n",
    "2. Zone properties (name, capacity, restricted)\n",
    "3. Track-to-zone mapping\n",
    "4. Occupancy counters per zone\n",
    "5. Alert triggers\n",
    "\n",
    "Implementation Steps:\n",
    "1. Define zone polygon\n",
    "2. Check if point is inside polygon (cv2.pointPolygonTest)\n",
    "3. Track which objects in which zones\n",
    "4. Count occupancy\n",
    "5. Trigger alerts on violations\n",
    "\"\"\"\n",
    "\n",
    "class Zone:\n",
    "    \"\"\"\n",
    "    Represents a monitoring zone (polygon area)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, vertices, capacity=None, restricted=False, color=(255, 0, 0)):\n",
    "        \"\"\"\n",
    "        Initialize zone\n",
    "        \n",
    "        Args:\n",
    "            name: Zone name (e.g., \"Meeting Room A\")\n",
    "            vertices: List of (x, y) points defining polygon\n",
    "            capacity: Max occupancy (None = unlimited)\n",
    "            restricted: Whether zone requires authorization\n",
    "            color: BGR color for visualization\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.vertices = np.array(vertices, dtype=np.int32)\n",
    "        self.capacity = capacity\n",
    "        self.restricted = restricted\n",
    "        self.color = color\n",
    "        \n",
    "        # Tracking\n",
    "        self.current_occupants = set()  # Track IDs currently in zone\n",
    "        self.entry_times = {}  # Track ID -> entry timestamp\n",
    "        self.total_entries = 0\n",
    "        self.total_exits = 0\n",
    "        \n",
    "        print(f\"‚úÖ Zone created: {name}\")\n",
    "        print(f\"   Vertices: {len(vertices)} points\")\n",
    "        print(f\"   Capacity: {capacity if capacity else 'Unlimited'}\")\n",
    "        print(f\"   Restricted: {restricted}\")\n",
    "    \n",
    "    def contains_point(self, point):\n",
    "        \"\"\"\n",
    "        Check if point is inside zone polygon\n",
    "        \n",
    "        Args:\n",
    "            point: (x, y) tuple\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if inside, False if outside\n",
    "        \"\"\"\n",
    "        result = cv2.pointPolygonTest(self.vertices, point, False)\n",
    "        return result >= 0  # >= 0 means inside or on edge\n",
    "    \n",
    "    def update(self, tracks):\n",
    "        \"\"\"\n",
    "        Update zone occupancy based on current tracks\n",
    "        \n",
    "        Args:\n",
    "            tracks: List of tracks from DeepSORT\n",
    "            \n",
    "        Returns:\n",
    "            dict: Events {track_id: event_type}\n",
    "        \"\"\"\n",
    "        events = {}\n",
    "        current_frame_occupants = set()\n",
    "        \n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            \n",
    "            track_id = track.track_id\n",
    "            bbox = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            \n",
    "            # Use bottom-center of bbox (person's feet position)\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int(y2)\n",
    "            point = (center_x, center_y)\n",
    "            \n",
    "            # Check if inside zone\n",
    "            if self.contains_point(point):\n",
    "                current_frame_occupants.add(track_id)\n",
    "                \n",
    "                # New entry?\n",
    "                if track_id not in self.current_occupants:\n",
    "                    self.current_occupants.add(track_id)\n",
    "                    self.entry_times[track_id] = time.time()\n",
    "                    self.total_entries += 1\n",
    "                    events[track_id] = \"ENTERED\"\n",
    "        \n",
    "        # Check for exits\n",
    "        exited = self.current_occupants - current_frame_occupants\n",
    "        for track_id in exited:\n",
    "            self.current_occupants.discard(track_id)\n",
    "            if track_id in self.entry_times:\n",
    "                del self.entry_times[track_id]\n",
    "            self.total_exits += 1\n",
    "            events[track_id] = \"EXITED\"\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def get_occupancy(self):\n",
    "        \"\"\"Get current occupancy count\"\"\"\n",
    "        return len(self.current_occupants)\n",
    "    \n",
    "    def is_over_capacity(self):\n",
    "        \"\"\"Check if zone is over capacity\"\"\"\n",
    "        if self.capacity is None:\n",
    "            return False\n",
    "        return self.get_occupancy() > self.capacity\n",
    "    \n",
    "    def get_dwell_times(self):\n",
    "        \"\"\"Get dwell times for current occupants\"\"\"\n",
    "        current_time = time.time()\n",
    "        return {\n",
    "            track_id: current_time - entry_time\n",
    "            for track_id, entry_time in self.entry_times.items()\n",
    "        }\n",
    "    \n",
    "    def draw(self, frame, show_info=True):\n",
    "        \"\"\"\n",
    "        Draw zone on frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            show_info: Whether to show zone info text\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame\n",
    "        \"\"\"\n",
    "        annotated = frame.copy()\n",
    "        \n",
    "        # Draw polygon with transparency\n",
    "        overlay = annotated.copy()\n",
    "        cv2.fillPoly(overlay, [self.vertices], self.color)\n",
    "        cv2.addWeighted(overlay, 0.3, annotated, 0.7, 0, annotated)\n",
    "        \n",
    "        # Draw border\n",
    "        cv2.polylines(annotated, [self.vertices], True, self.color, 2)\n",
    "        \n",
    "        if show_info:\n",
    "            # Calculate text position (centroid of polygon)\n",
    "            M = cv2.moments(self.vertices)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "            else:\n",
    "                cx, cy = self.vertices[0]\n",
    "            \n",
    "            # Zone name\n",
    "            cv2.putText(annotated, self.name, (cx - 50, cy - 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Occupancy\n",
    "            occupancy = self.get_occupancy()\n",
    "            capacity_text = f\"/{self.capacity}\" if self.capacity else \"\"\n",
    "            occupancy_text = f\"Occupancy: {occupancy}{capacity_text}\"\n",
    "            \n",
    "            # Color based on capacity\n",
    "            if self.is_over_capacity():\n",
    "                text_color = (0, 0, 255)  # Red if over\n",
    "            else:\n",
    "                text_color = (0, 255, 0)  # Green if OK\n",
    "            \n",
    "            cv2.putText(annotated, occupancy_text, (cx - 50, cy),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)\n",
    "            \n",
    "            # Restricted label\n",
    "            if self.restricted:\n",
    "                cv2.putText(annotated, \"RESTRICTED\", (cx - 50, cy + 25),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        return annotated\n",
    "\n",
    "print(\"\\n‚úÖ Class created: Zone\")\n",
    "print(\"\\nüìä Features:\")\n",
    "print(\"   ‚Ä¢ Polygon-based zone definition\")\n",
    "print(\"   ‚Ä¢ Point-in-polygon detection (cv2)\")\n",
    "print(\"   ‚Ä¢ Occupancy tracking with entry/exit events\")\n",
    "print(\"   ‚Ä¢ Capacity monitoring and alerts\")\n",
    "print(\"   ‚Ä¢ Dwell time calculation\")\n",
    "print(\"   ‚Ä¢ Visual overlay with transparency\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 3.2 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ad2e9f-8f19-4c37-b5ba-46ea3aff7850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 3.3: Zone Monitoring Demo Code\n",
      "================================================================================\n",
      "\n",
      "üè¢ COMPLETE ZONE MONITORING DEMO\n",
      "\n",
      "Below is the full code for zone-based monitoring.\n",
      "This combines tracking + zones + alerts!\n",
      "\n",
      "Features:\n",
      "‚úì Multiple zones (meeting room, restricted area, etc.)\n",
      "‚úì Real-time occupancy per zone\n",
      "‚úì Capacity alerts (over limit)\n",
      "‚úì Restricted area alerts (unauthorized access)\n",
      "‚úì Visual zone overlays\n",
      "‚úì Entry/exit events\n",
      "\n",
      "üìù To run this demo:\n",
      "1. Copy the code below to a new cell\n",
      "2. Adjust zone coordinates for your camera view\n",
      "3. Execute the cell\n",
      "4. Walk in/out of zones to test!\n",
      "5. Press 'q' to quit\n",
      "\n",
      "‚ö†Ô∏è  Note: Zone coordinates are for 640x480 resolution\n",
      "         Adjust vertices based on your camera!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CODE: ZONE MONITORING DEMO\n",
      "================================================================================\n",
      "\n",
      "Copy this code to a new cell to run zone monitoring:\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "import cv2\n",
      "import numpy as np\n",
      "from ultralytics import YOLO\n",
      "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
      "import time\n",
      "\n",
      "# Load models\n",
      "print(\"Loading models...\")\n",
      "model = YOLO('yolov8n.pt')\n",
      "tracker = DeepSort(max_age=30, n_init=3, nms_max_overlap=1.0, embedder=None)\n",
      "\n",
      "# Get camera dimensions\n",
      "cap_temp = cv2.VideoCapture(0)\n",
      "ret, frame_temp = cap_temp.read()\n",
      "height, width = frame_temp.shape[:2]\n",
      "cap_temp.release()\n",
      "\n",
      "print(f\"Camera: {width}x{height}\")\n",
      "\n",
      "# Define zones (adjust these coordinates for your camera!)\n",
      "zones = [\n",
      "    Zone(\n",
      "        name=\"Meeting Room\",\n",
      "        vertices=[\n",
      "            (100, 100),   # Top-left\n",
      "            (300, 100),   # Top-right\n",
      "            (300, 300),   # Bottom-right\n",
      "            (100, 300)    # Bottom-left\n",
      "        ],\n",
      "        capacity=2,\n",
      "        restricted=False,\n",
      "        color=(0, 255, 0)  # Green\n",
      "    ),\n",
      "    Zone(\n",
      "        name=\"Restricted Area\",\n",
      "        vertices=[\n",
      "            (width-300, 100),\n",
      "            (width-100, 100),\n",
      "            (width-100, 300),\n",
      "            (width-300, 300)\n",
      "        ],\n",
      "        capacity=1,\n",
      "        restricted=True,\n",
      "        color=(0, 0, 255)  # Red\n",
      "    ),\n",
      "]\n",
      "\n",
      "print(f\"\\nZones created: {len(zones)}\")\n",
      "\n",
      "# Open webcam\n",
      "cap = cv2.VideoCapture(0)\n",
      "\n",
      "if not cap.isOpened():\n",
      "    print(\"‚ùå Cannot open webcam!\")\n",
      "else:\n",
      "    print(\"‚úÖ Webcam opened!\")\n",
      "    print(\"üé• Starting zone monitoring... (Press 'q' to quit)\")\n",
      "\n",
      "    fps_counter = 0\n",
      "    start_time = time.time()\n",
      "    fps = 0\n",
      "\n",
      "    # Alert history\n",
      "    alerts = []\n",
      "\n",
      "    while True:\n",
      "        ret, frame = cap.read()\n",
      "        if not ret:\n",
      "            break\n",
      "\n",
      "        # 1. YOLO Detection\n",
      "        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)\n",
      "        detections = results[0].boxes\n",
      "\n",
      "        # 2. Convert to DeepSORT\n",
      "        deepsort_input = []\n",
      "        for box in detections:\n",
      "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
      "            conf = float(box.conf[0])\n",
      "            w, h = x2 - x1, y2 - y1\n",
      "            deepsort_input.append(([x1, y1, w, h], conf, 'person'))\n",
      "\n",
      "        # 3. Update tracker\n",
      "        if len(deepsort_input) > 0:\n",
      "            dummy_embeddings = [np.random.rand(128).astype(np.float32) for _ in deepsort_input]\n",
      "            tracks = tracker.update_tracks(deepsort_input, embeds=dummy_embeddings, frame=frame)\n",
      "        else:\n",
      "            tracks = []\n",
      "\n",
      "        # 4. Update zones\n",
      "        for zone in zones:\n",
      "            events = zone.update(tracks)\n",
      "\n",
      "            # Check for alerts\n",
      "            for track_id, event in events.items():\n",
      "                if event == \"ENTERED\":\n",
      "                    alert_msg = f\"ID {track_id} entered {zone.name}\"\n",
      "                    alerts.append((time.time(), alert_msg))\n",
      "                    print(f\"üö™ {alert_msg}\")\n",
      "\n",
      "            # Capacity alert\n",
      "            if zone.is_over_capacity():\n",
      "                alert_msg = f\"{zone.name} OVER CAPACITY ({zone.get_occupancy()}/{zone.capacity})\"\n",
      "                if not any(alert_msg in a[1] for a in alerts[-5:]):  # Avoid spam\n",
      "                    alerts.append((time.time(), alert_msg))\n",
      "                    print(f\"‚ö†Ô∏è  {alert_msg}\")\n",
      "\n",
      "        # 5. Draw zones\n",
      "        for zone in zones:\n",
      "            frame = zone.draw(frame)\n",
      "\n",
      "        # 6. Visualize tracks\n",
      "        for track in tracks:\n",
      "            if not track.is_confirmed():\n",
      "                continue\n",
      "\n",
      "            track_id = track.track_id\n",
      "            bbox = track.to_ltrb()\n",
      "            x1, y1, x2, y2 = map(int, bbox)\n",
      "\n",
      "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
      "            cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10),\n",
      "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
      "\n",
      "        # 7. Display recent alerts\n",
      "        y_offset = 30\n",
      "        for i, (alert_time, alert_msg) in enumerate(alerts[-5:]):  # Last 5 alerts\n",
      "            age = time.time() - alert_time\n",
      "            if age < 5:  # Show for 5 seconds\n",
      "                alpha = 1 - (age / 5)  # Fade out\n",
      "                cv2.putText(frame, f\"‚ö†Ô∏è {alert_msg}\", (10, y_offset),\n",
      "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
      "                y_offset += 25\n",
      "\n",
      "        # 8. Calculate FPS\n",
      "        fps_counter += 1\n",
      "        if fps_counter % 30 == 0:\n",
      "            elapsed = time.time() - start_time\n",
      "            fps = 30 / elapsed\n",
      "            start_time = time.time()\n",
      "\n",
      "        # 9. Display FPS\n",
      "        cv2.putText(frame, f'FPS: {fps:.1f}', (width - 150, 30),\n",
      "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
      "\n",
      "        # 10. Show frame\n",
      "        cv2.imshow('Zone Monitoring System', frame)\n",
      "\n",
      "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "            break\n",
      "\n",
      "    cap.release()\n",
      "    cv2.destroyAllWindows()\n",
      "\n",
      "    # Final statistics\n",
      "    print(f\"\\nüìä Final Zone Statistics:\")\n",
      "    for zone in zones:\n",
      "        print(f\"\\n{zone.name}:\")\n",
      "        print(f\"   ‚Ä¢ Total entries: {zone.total_entries}\")\n",
      "        print(f\"   ‚Ä¢ Total exits: {zone.total_exits}\")\n",
      "        print(f\"   ‚Ä¢ Current occupancy: {zone.get_occupancy()}\")\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üí° What to Observe:\n",
      "   ‚Ä¢ Colored zone overlays (green, red)\n",
      "   ‚Ä¢ Zone names and occupancy displayed\n",
      "   ‚Ä¢ Alerts appear when entering zones\n",
      "   ‚Ä¢ Capacity warnings if zone over limit\n",
      "   ‚Ä¢ Entry/exit events logged\n",
      "\n",
      "üß™ Test Scenarios:\n",
      "   1. Walk into green zone (meeting room)\n",
      "   2. Stay in zone (see occupancy count)\n",
      "   3. Walk into red zone (restricted area)\n",
      "   4. Exceed capacity (enter with 2+ people)\n",
      "   5. Walk out (exit event, occupancy decreases)\n",
      "\n",
      "‚úÖ Exercise 3.3 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 3.3: ZONE MONITORING DEMO\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 3.3: Zone Monitoring Demo Code\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "üìñ THEORY: Complete Zone Monitoring System\n",
    "\n",
    "Integration:\n",
    "1. Define zones (polygons)\n",
    "2. Track objects with DeepSORT\n",
    "3. Check which zone each object is in\n",
    "4. Update zone occupancy\n",
    "5. Detect violations (over capacity, restricted access)\n",
    "6. Visualize zones and alerts\n",
    "\n",
    "Real-world Use Cases:\n",
    "- Meeting room capacity monitoring\n",
    "- Restricted area access control\n",
    "- Safety zone monitoring (hazard areas)\n",
    "- Queue management\n",
    "- Traffic flow analysis\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "üè¢ COMPLETE ZONE MONITORING DEMO\n",
    "\n",
    "Below is the full code for zone-based monitoring.\n",
    "This combines tracking + zones + alerts!\n",
    "\n",
    "Features:\n",
    "‚úì Multiple zones (meeting room, restricted area, etc.)\n",
    "‚úì Real-time occupancy per zone\n",
    "‚úì Capacity alerts (over limit)\n",
    "‚úì Restricted area alerts (unauthorized access)\n",
    "‚úì Visual zone overlays\n",
    "‚úì Entry/exit events\n",
    "\n",
    "üìù To run this demo:\n",
    "1. Copy the code below to a new cell\n",
    "2. Adjust zone coordinates for your camera view\n",
    "3. Execute the cell\n",
    "4. Walk in/out of zones to test!\n",
    "5. Press 'q' to quit\n",
    "\n",
    "‚ö†Ô∏è  Note: Zone coordinates are for 640x480 resolution\n",
    "         Adjust vertices based on your camera!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CODE: ZONE MONITORING DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Copy this code to a new cell to run zone monitoring:\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "model = YOLO('yolov8n.pt')\n",
    "tracker = DeepSort(max_age=30, n_init=3, nms_max_overlap=1.0, embedder=None)\n",
    "\n",
    "# Get camera dimensions\n",
    "cap_temp = cv2.VideoCapture(0)\n",
    "ret, frame_temp = cap_temp.read()\n",
    "height, width = frame_temp.shape[:2]\n",
    "cap_temp.release()\n",
    "\n",
    "print(f\"Camera: {width}x{height}\")\n",
    "\n",
    "# Define zones (adjust these coordinates for your camera!)\n",
    "zones = [\n",
    "    Zone(\n",
    "        name=\"Meeting Room\",\n",
    "        vertices=[\n",
    "            (100, 100),   # Top-left\n",
    "            (300, 100),   # Top-right\n",
    "            (300, 300),   # Bottom-right\n",
    "            (100, 300)    # Bottom-left\n",
    "        ],\n",
    "        capacity=2,\n",
    "        restricted=False,\n",
    "        color=(0, 255, 0)  # Green\n",
    "    ),\n",
    "    Zone(\n",
    "        name=\"Restricted Area\",\n",
    "        vertices=[\n",
    "            (width-300, 100),\n",
    "            (width-100, 100),\n",
    "            (width-100, 300),\n",
    "            (width-300, 300)\n",
    "        ],\n",
    "        capacity=1,\n",
    "        restricted=True,\n",
    "        color=(0, 0, 255)  # Red\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"\\\\nZones created: {len(zones)}\")\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Cannot open webcam!\")\n",
    "else:\n",
    "    print(\"‚úÖ Webcam opened!\")\n",
    "    print(\"üé• Starting zone monitoring... (Press 'q' to quit)\")\n",
    "    \n",
    "    fps_counter = 0\n",
    "    start_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    # Alert history\n",
    "    alerts = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 1. YOLO Detection\n",
    "        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)\n",
    "        detections = results[0].boxes\n",
    "        \n",
    "        # 2. Convert to DeepSORT\n",
    "        deepsort_input = []\n",
    "        for box in detections:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = float(box.conf[0])\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            deepsort_input.append(([x1, y1, w, h], conf, 'person'))\n",
    "        \n",
    "        # 3. Update tracker\n",
    "        if len(deepsort_input) > 0:\n",
    "            dummy_embeddings = [np.random.rand(128).astype(np.float32) for _ in deepsort_input]\n",
    "            tracks = tracker.update_tracks(deepsort_input, embeds=dummy_embeddings, frame=frame)\n",
    "        else:\n",
    "            tracks = []\n",
    "        \n",
    "        # 4. Update zones\n",
    "        for zone in zones:\n",
    "            events = zone.update(tracks)\n",
    "            \n",
    "            # Check for alerts\n",
    "            for track_id, event in events.items():\n",
    "                if event == \"ENTERED\":\n",
    "                    alert_msg = f\"ID {track_id} entered {zone.name}\"\n",
    "                    alerts.append((time.time(), alert_msg))\n",
    "                    print(f\"üö™ {alert_msg}\")\n",
    "            \n",
    "            # Capacity alert\n",
    "            if zone.is_over_capacity():\n",
    "                alert_msg = f\"{zone.name} OVER CAPACITY ({zone.get_occupancy()}/{zone.capacity})\"\n",
    "                if not any(alert_msg in a[1] for a in alerts[-5:]):  # Avoid spam\n",
    "                    alerts.append((time.time(), alert_msg))\n",
    "                    print(f\"‚ö†Ô∏è  {alert_msg}\")\n",
    "        \n",
    "        # 5. Draw zones\n",
    "        for zone in zones:\n",
    "            frame = zone.draw(frame)\n",
    "        \n",
    "        # 6. Visualize tracks\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            \n",
    "            track_id = track.track_id\n",
    "            bbox = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # 7. Display recent alerts\n",
    "        y_offset = 30\n",
    "        for i, (alert_time, alert_msg) in enumerate(alerts[-5:]):  # Last 5 alerts\n",
    "            age = time.time() - alert_time\n",
    "            if age < 5:  # Show for 5 seconds\n",
    "                alpha = 1 - (age / 5)  # Fade out\n",
    "                cv2.putText(frame, f\"‚ö†Ô∏è {alert_msg}\", (10, y_offset),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "                y_offset += 25\n",
    "        \n",
    "        # 8. Calculate FPS\n",
    "        fps_counter += 1\n",
    "        if fps_counter % 30 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            fps = 30 / elapsed\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # 9. Display FPS\n",
    "        cv2.putText(frame, f'FPS: {fps:.1f}', (width - 150, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        # 10. Show frame\n",
    "        cv2.imshow('Zone Monitoring System', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Final statistics\n",
    "    print(f\"\\\\nüìä Final Zone Statistics:\")\n",
    "    for zone in zones:\n",
    "        print(f\"\\\\n{zone.name}:\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {zone.total_entries}\")\n",
    "        print(f\"   ‚Ä¢ Total exits: {zone.total_exits}\")\n",
    "        print(f\"   ‚Ä¢ Current occupancy: {zone.get_occupancy()}\")\n",
    "-----------------------------------------------------------------------\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° What to Observe:\")\n",
    "print(\"   ‚Ä¢ Colored zone overlays (green, red)\")\n",
    "print(\"   ‚Ä¢ Zone names and occupancy displayed\")\n",
    "print(\"   ‚Ä¢ Alerts appear when entering zones\")\n",
    "print(\"   ‚Ä¢ Capacity warnings if zone over limit\")\n",
    "print(\"   ‚Ä¢ Entry/exit events logged\")\n",
    "\n",
    "print(\"\\nüß™ Test Scenarios:\")\n",
    "print(\"   1. Walk into green zone (meeting room)\")\n",
    "print(\"   2. Stay in zone (see occupancy count)\")\n",
    "print(\"   3. Walk into red zone (restricted area)\")\n",
    "print(\"   4. Exceed capacity (enter with 2+ people)\")\n",
    "print(\"   5. Walk out (exit event, occupancy decreases)\")\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 3.3 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ff248e-0bbf-4d85-94b8-9200c487a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ PART 4: KEY TAKEAWAYS & NEXT STEPS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ PART 4: KEY TAKEAWAYS & NEXT STEPS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b425c301-0c84-419a-82c0-e8542b4a22bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 4.1: Day 24 Summary\n",
      "================================================================================\n",
      "\n",
      "üìö WHAT WE LEARNED TODAY:\n",
      "\n",
      "‚úÖ DeepSORT Parameter Tuning:\n",
      "   ‚Ä¢ Understood max_age (occlusion tolerance)\n",
      "   ‚Ä¢ Understood n_init (confirmation threshold)\n",
      "   ‚Ä¢ Understood max_iou_distance (matching strictness)\n",
      "   ‚Ä¢ Created testing framework for parameter comparison\n",
      "   ‚Ä¢ Learned how to tune for different scenarios\n",
      "\n",
      "‚úÖ Occlusion Handling:\n",
      "   ‚Ä¢ Learned how Kalman filter predicts during occlusion\n",
      "   ‚Ä¢ Understood track state management (tentative, confirmed, lost)\n",
      "   ‚Ä¢ Learned appearance matching for re-identification\n",
      "   ‚Ä¢ Simulated occlusion scenarios\n",
      "   ‚Ä¢ Determined optimal max_age values\n",
      "\n",
      "‚úÖ Line-Crossing Detection:\n",
      "   ‚Ä¢ Implemented cross-product method for line crossing\n",
      "   ‚Ä¢ Created PeopleCounter class with direction detection\n",
      "   ‚Ä¢ Handled edge cases (oscillation, lingering)\n",
      "   ‚Ä¢ Built bidirectional counting (IN vs OUT)\n",
      "   ‚Ä¢ Calculated current occupancy (IN - OUT)\n",
      "\n",
      "‚úÖ Zone-Based Monitoring:\n",
      "   ‚Ä¢ Implemented polygon zone definition\n",
      "   ‚Ä¢ Used point-in-polygon detection (cv2)\n",
      "   ‚Ä¢ Created Zone class with occupancy tracking\n",
      "   ‚Ä¢ Implemented entry/exit events\n",
      "   ‚Ä¢ Built capacity monitoring and alerts\n",
      "   ‚Ä¢ Calculated dwell times per zone\n",
      "\n",
      "‚úÖ Webcam Testing:\n",
      "   ‚Ä¢ Tested tracking on MacBook webcam\n",
      "   ‚Ä¢ Verified line-crossing detection works\n",
      "   ‚Ä¢ Tracked multiple object types (people, cats)\n",
      "   ‚Ä¢ Color-coded visualization working\n",
      "   ‚Ä¢ Achieved real-time performance\n",
      "\n",
      "üìä KEY METRICS TODAY:\n",
      "   ‚Ä¢ Parameter configurations tested: 5\n",
      "   ‚Ä¢ Occlusion scenarios analyzed: 4\n",
      "   ‚Ä¢ Line-crossing accuracy: ~95%\n",
      "   ‚Ä¢ Zone monitoring classes created: 2\n",
      "   ‚Ä¢ Webcam test: Successful ‚úì\n",
      "   ‚Ä¢ FPS achieved: 20-30 (CPU)\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "\n",
      "   1. Parameter tuning is scenario-dependent\n",
      "      ‚Üí Office: max_age=20-30, n_init=2-3\n",
      "      ‚Üí Factory: max_age=40-60, n_init=4-5\n",
      "      ‚Üí Adjust based on real deployment conditions\n",
      "\n",
      "   2. Line-crossing is simple but powerful\n",
      "      ‚Üí Cross-product method is efficient\n",
      "      ‚Üí Direction detection enables traffic analysis\n",
      "      ‚Üí Handles multiple people simultaneously\n",
      "\n",
      "   3. Zone monitoring adds spatial context\n",
      "      ‚Üí Polygon zones more flexible than rectangles\n",
      "      ‚Üí Occupancy tracking enables capacity management\n",
      "      ‚Üí Dwell time useful for behavior analysis\n",
      "\n",
      "   4. Tracking works without embedder\n",
      "      ‚Üí Dummy embeddings sufficient for basic tracking\n",
      "      ‚Üí Appearance features improve re-ID but not critical\n",
      "      ‚Üí Motion-based tracking handles most scenarios\n",
      "\n",
      "   5. Real-time performance achievable\n",
      "      ‚Üí 20-30 FPS on CPU is usable\n",
      "      ‚Üí Visualization overhead is minimal\n",
      "      ‚Üí GPU would give 40-60 FPS\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Exercise 4.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 4.1: DAY 24 SUMMARY\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 4.1: Day 24 Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üìö WHAT WE LEARNED TODAY:\n",
    "\n",
    "‚úÖ DeepSORT Parameter Tuning:\n",
    "   ‚Ä¢ Understood max_age (occlusion tolerance)\n",
    "   ‚Ä¢ Understood n_init (confirmation threshold)\n",
    "   ‚Ä¢ Understood max_iou_distance (matching strictness)\n",
    "   ‚Ä¢ Created testing framework for parameter comparison\n",
    "   ‚Ä¢ Learned how to tune for different scenarios\n",
    "\n",
    "‚úÖ Occlusion Handling:\n",
    "   ‚Ä¢ Learned how Kalman filter predicts during occlusion\n",
    "   ‚Ä¢ Understood track state management (tentative, confirmed, lost)\n",
    "   ‚Ä¢ Learned appearance matching for re-identification\n",
    "   ‚Ä¢ Simulated occlusion scenarios\n",
    "   ‚Ä¢ Determined optimal max_age values\n",
    "\n",
    "‚úÖ Line-Crossing Detection:\n",
    "   ‚Ä¢ Implemented cross-product method for line crossing\n",
    "   ‚Ä¢ Created PeopleCounter class with direction detection\n",
    "   ‚Ä¢ Handled edge cases (oscillation, lingering)\n",
    "   ‚Ä¢ Built bidirectional counting (IN vs OUT)\n",
    "   ‚Ä¢ Calculated current occupancy (IN - OUT)\n",
    "\n",
    "‚úÖ Zone-Based Monitoring:\n",
    "   ‚Ä¢ Implemented polygon zone definition\n",
    "   ‚Ä¢ Used point-in-polygon detection (cv2)\n",
    "   ‚Ä¢ Created Zone class with occupancy tracking\n",
    "   ‚Ä¢ Implemented entry/exit events\n",
    "   ‚Ä¢ Built capacity monitoring and alerts\n",
    "   ‚Ä¢ Calculated dwell times per zone\n",
    "\n",
    "‚úÖ Webcam Testing:\n",
    "   ‚Ä¢ Tested tracking on MacBook webcam\n",
    "   ‚Ä¢ Verified line-crossing detection works\n",
    "   ‚Ä¢ Tracked multiple object types (people, cats)\n",
    "   ‚Ä¢ Color-coded visualization working\n",
    "   ‚Ä¢ Achieved real-time performance\n",
    "\n",
    "üìä KEY METRICS TODAY:\n",
    "   ‚Ä¢ Parameter configurations tested: 5\n",
    "   ‚Ä¢ Occlusion scenarios analyzed: 4\n",
    "   ‚Ä¢ Line-crossing accuracy: ~95%\n",
    "   ‚Ä¢ Zone monitoring classes created: 2\n",
    "   ‚Ä¢ Webcam test: Successful ‚úì\n",
    "   ‚Ä¢ FPS achieved: 20-30 (CPU)\n",
    "\n",
    "üí° KEY INSIGHTS:\n",
    "\n",
    "   1. Parameter tuning is scenario-dependent\n",
    "      ‚Üí Office: max_age=20-30, n_init=2-3\n",
    "      ‚Üí Factory: max_age=40-60, n_init=4-5\n",
    "      ‚Üí Adjust based on real deployment conditions\n",
    "      \n",
    "   2. Line-crossing is simple but powerful\n",
    "      ‚Üí Cross-product method is efficient\n",
    "      ‚Üí Direction detection enables traffic analysis\n",
    "      ‚Üí Handles multiple people simultaneously\n",
    "      \n",
    "   3. Zone monitoring adds spatial context\n",
    "      ‚Üí Polygon zones more flexible than rectangles\n",
    "      ‚Üí Occupancy tracking enables capacity management\n",
    "      ‚Üí Dwell time useful for behavior analysis\n",
    "      \n",
    "   4. Tracking works without embedder\n",
    "      ‚Üí Dummy embeddings sufficient for basic tracking\n",
    "      ‚Üí Appearance features improve re-ID but not critical\n",
    "      ‚Üí Motion-based tracking handles most scenarios\n",
    "      \n",
    "   5. Real-time performance achievable\n",
    "      ‚Üí 20-30 FPS on CPU is usable\n",
    "      ‚Üí Visualization overhead is minimal\n",
    "      ‚Üí GPU would give 40-60 FPS\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 4.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb7cc3f-384d-4959-ad48-3d873a5c5a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 4.2: Tomorrow's Plan\n",
      "================================================================================\n",
      "\n",
      "üéØ DAY 25: VIDEO PROCESSING PIPELINE (November 20, 2025)\n",
      "\n",
      "What we'll do:\n",
      "1. Build production video processing pipeline\n",
      "   ‚Ä¢ Multi-threaded video reading\n",
      "   ‚Ä¢ Frame queue management\n",
      "   ‚Ä¢ FPS control and synchronization\n",
      "   ‚Ä¢ Memory management\n",
      "\n",
      "2. Handle multiple video sources\n",
      "   ‚Ä¢ Video files (MP4, AVI, MOV)\n",
      "   ‚Ä¢ Webcam input\n",
      "   ‚Ä¢ RTSP streams (IP cameras)\n",
      "   ‚Ä¢ Multiple cameras simultaneously\n",
      "\n",
      "3. Implement frame skipping strategies\n",
      "   ‚Ä¢ Skip every Nth frame if needed\n",
      "   ‚Ä¢ Adaptive frame rate based on detection load\n",
      "   ‚Ä¢ Balance accuracy vs speed\n",
      "\n",
      "4. Output video processing\n",
      "   ‚Ä¢ Save annotated videos\n",
      "   ‚Ä¢ Export detection logs (CSV, JSON)\n",
      "   ‚Ä¢ Generate summary reports\n",
      "   ‚Ä¢ Create highlight clips\n",
      "\n",
      "5. Batch processing capabilities\n",
      "   ‚Ä¢ Process multiple videos\n",
      "   ‚Ä¢ Parallel processing\n",
      "   ‚Ä¢ Progress tracking\n",
      "   ‚Ä¢ Error handling\n",
      "\n",
      "6. Performance optimization\n",
      "   ‚Ä¢ GPU acceleration (if available)\n",
      "   ‚Ä¢ Batch inference\n",
      "   ‚Ä¢ Resolution optimization\n",
      "   ‚Ä¢ Memory leak prevention\n",
      "\n",
      "Expected outcomes:\n",
      "   ‚Ä¢ Robust video processing pipeline\n",
      "   ‚Ä¢ Handle various input formats\n",
      "   ‚Ä¢ Process videos efficiently\n",
      "   ‚Ä¢ Export results in multiple formats\n",
      "   ‚Ä¢ 30 FPS sustained performance\n",
      "   ‚Ä¢ Production-ready code structure\n",
      "\n",
      "Tech Stack:\n",
      "   ‚Ä¢ OpenCV (video I/O)\n",
      "   ‚Ä¢ Threading/multiprocessing (parallel processing)\n",
      "   ‚Ä¢ Queue (frame buffering)\n",
      "   ‚Ä¢ JSON/CSV (export formats)\n",
      "   ‚Ä¢ tqdm (progress bars)\n",
      "\n",
      "Time estimate: 5-6 hours\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Exercise 4.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 4.2: TOMORROW'S PLAN (DAY 25)\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 4.2: Tomorrow's Plan\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ DAY 25: VIDEO PROCESSING PIPELINE (November 20, 2025)\n",
    "\n",
    "What we'll do:\n",
    "1. Build production video processing pipeline\n",
    "   ‚Ä¢ Multi-threaded video reading\n",
    "   ‚Ä¢ Frame queue management\n",
    "   ‚Ä¢ FPS control and synchronization\n",
    "   ‚Ä¢ Memory management\n",
    "\n",
    "2. Handle multiple video sources\n",
    "   ‚Ä¢ Video files (MP4, AVI, MOV)\n",
    "   ‚Ä¢ Webcam input\n",
    "   ‚Ä¢ RTSP streams (IP cameras)\n",
    "   ‚Ä¢ Multiple cameras simultaneously\n",
    "\n",
    "3. Implement frame skipping strategies\n",
    "   ‚Ä¢ Skip every Nth frame if needed\n",
    "   ‚Ä¢ Adaptive frame rate based on detection load\n",
    "   ‚Ä¢ Balance accuracy vs speed\n",
    "\n",
    "4. Output video processing\n",
    "   ‚Ä¢ Save annotated videos\n",
    "   ‚Ä¢ Export detection logs (CSV, JSON)\n",
    "   ‚Ä¢ Generate summary reports\n",
    "   ‚Ä¢ Create highlight clips\n",
    "\n",
    "5. Batch processing capabilities\n",
    "   ‚Ä¢ Process multiple videos\n",
    "   ‚Ä¢ Parallel processing\n",
    "   ‚Ä¢ Progress tracking\n",
    "   ‚Ä¢ Error handling\n",
    "\n",
    "6. Performance optimization\n",
    "   ‚Ä¢ GPU acceleration (if available)\n",
    "   ‚Ä¢ Batch inference\n",
    "   ‚Ä¢ Resolution optimization\n",
    "   ‚Ä¢ Memory leak prevention\n",
    "\n",
    "Expected outcomes:\n",
    "   ‚Ä¢ Robust video processing pipeline\n",
    "   ‚Ä¢ Handle various input formats\n",
    "   ‚Ä¢ Process videos efficiently\n",
    "   ‚Ä¢ Export results in multiple formats\n",
    "   ‚Ä¢ 30 FPS sustained performance\n",
    "   ‚Ä¢ Production-ready code structure\n",
    "\n",
    "Tech Stack:\n",
    "   ‚Ä¢ OpenCV (video I/O)\n",
    "   ‚Ä¢ Threading/multiprocessing (parallel processing)\n",
    "   ‚Ä¢ Queue (frame buffering)\n",
    "   ‚Ä¢ JSON/CSV (export formats)\n",
    "   ‚Ä¢ tqdm (progress bars)\n",
    "\n",
    "Time estimate: 5-6 hours\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Exercise 4.2 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518d97ca-8474-4153-98bf-7fabe15c8a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DAY 24 COMPLETE! ‚úÖ\n",
      "================================================================================\n",
      "\n",
      "OBJECTIVES ACHIEVED:\n",
      "   ‚úÖ Tuned DeepSORT parameters for different scenarios\n",
      "   ‚úÖ Handled occlusions robustly (max_age optimization)\n",
      "   ‚úÖ Implemented line-crossing detection system\n",
      "   ‚úÖ Created PeopleCounter class with bidirectional counting\n",
      "   ‚úÖ Built zone-based monitoring system\n",
      "   ‚úÖ Implemented Zone class with capacity management\n",
      "   ‚úÖ Tested on webcam (MacBook - successful!)\n",
      "   ‚úÖ Tracked multiple object types (people, cats, etc.)\n",
      "   ‚úÖ Achieved real-time performance (20-30 FPS)\n",
      "\n",
      "üìä KEY METRICS:\n",
      "   - Parameter configurations: 5 tested\n",
      "   - Counting accuracy: ~95%\n",
      "   - Zone detection: Point-in-polygon working\n",
      "   - Webcam FPS: 20-30 (CPU)\n",
      "   - Object types tracked: All 80 COCO classes\n",
      "   - Color visualization: 4 different colors\n",
      "\n",
      "üí° KEY LEARNINGS:\n",
      "   - Parameter tuning critical for different scenarios\n",
      "   - Line-crossing simple but effective for counting\n",
      "   - Zone monitoring adds powerful spatial awareness\n",
      "   - Tracking works well without complex embeddings\n",
      "   - Real-time performance achievable on CPU\n",
      "   - Webcam testing validates implementation\n",
      "   - Cross-product method elegant for line detection\n",
      "\n",
      "üéØ TOMORROW (DAY 25):\n",
      "   - Build production video processing pipeline\n",
      "   - Handle multiple input formats\n",
      "   - Implement batch processing\n",
      "   - Export results (CSV, JSON, video)\n",
      "   - Optimize for sustained 30 FPS\n",
      "   - Create modular, reusable code\n",
      "\n",
      "üíæ FILES CREATED TODAY:\n",
      "   - day24_tracking_optimization.ipynb (Complete!)\n",
      "   - Classes: TrackerConfig, PeopleCounter, Zone\n",
      "   - Functions: yolo_to_deepsort, simulate_occlusion_scenario\n",
      "   - Webcam demos: People counting, zone monitoring\n",
      "   - Theory: Parameters, occlusion, line-crossing, zones\n",
      "\n",
      "üî• PROGRESS UPDATE:\n",
      "   Week 4: 43% complete (3/7 days)\n",
      "   Overall: 14.3% complete (24/168 days)\n",
      "\n",
      "üöÄ MOMENTUM:\n",
      "   ‚úÖ Week 1: Neural Networks (Complete)\n",
      "   ‚úÖ Week 2: YOLO Detection (Complete - 75.1% mAP)\n",
      "   ‚úÖ Week 3: Medical Classifier (Complete - 94.48%)\n",
      "   ‚úÖ Day 22: Security System Planning (Complete)\n",
      "   ‚úÖ Day 23: DeepSORT Integration (Complete)\n",
      "   ‚úÖ Day 24: Tracking Optimization (Complete - TODAY!)\n",
      "\n",
      "   Next: Video processing pipeline with multiple sources! üé¨\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DAY 24 COMPLETE! ‚úÖ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "OBJECTIVES ACHIEVED:\n",
    "   ‚úÖ Tuned DeepSORT parameters for different scenarios\n",
    "   ‚úÖ Handled occlusions robustly (max_age optimization)\n",
    "   ‚úÖ Implemented line-crossing detection system\n",
    "   ‚úÖ Created PeopleCounter class with bidirectional counting\n",
    "   ‚úÖ Built zone-based monitoring system\n",
    "   ‚úÖ Implemented Zone class with capacity management\n",
    "   ‚úÖ Tested on webcam (MacBook - successful!)\n",
    "   ‚úÖ Tracked multiple object types (people, cats, etc.)\n",
    "   ‚úÖ Achieved real-time performance (20-30 FPS)\n",
    "\n",
    "üìä KEY METRICS:\n",
    "   - Parameter configurations: 5 tested\n",
    "   - Counting accuracy: ~95%\n",
    "   - Zone detection: Point-in-polygon working\n",
    "   - Webcam FPS: 20-30 (CPU)\n",
    "   - Object types tracked: All 80 COCO classes\n",
    "   - Color visualization: 4 different colors\n",
    "\n",
    "üí° KEY LEARNINGS:\n",
    "   - Parameter tuning critical for different scenarios\n",
    "   - Line-crossing simple but effective for counting\n",
    "   - Zone monitoring adds powerful spatial awareness\n",
    "   - Tracking works well without complex embeddings\n",
    "   - Real-time performance achievable on CPU\n",
    "   - Webcam testing validates implementation\n",
    "   - Cross-product method elegant for line detection\n",
    "\n",
    "üéØ TOMORROW (DAY 25):\n",
    "   - Build production video processing pipeline\n",
    "   - Handle multiple input formats\n",
    "   - Implement batch processing\n",
    "   - Export results (CSV, JSON, video)\n",
    "   - Optimize for sustained 30 FPS\n",
    "   - Create modular, reusable code\n",
    "\n",
    "üíæ FILES CREATED TODAY:\n",
    "   - day24_tracking_optimization.ipynb (Complete!)\n",
    "   - Classes: TrackerConfig, PeopleCounter, Zone\n",
    "   - Functions: yolo_to_deepsort, simulate_occlusion_scenario\n",
    "   - Webcam demos: People counting, zone monitoring\n",
    "   - Theory: Parameters, occlusion, line-crossing, zones\n",
    "\n",
    "üî• PROGRESS UPDATE:\n",
    "   Week 4: 43% complete (3/7 days)\n",
    "   Overall: 14.3% complete (24/168 days)\n",
    "   \n",
    "üöÄ MOMENTUM:\n",
    "   ‚úÖ Week 1: Neural Networks (Complete)\n",
    "   ‚úÖ Week 2: YOLO Detection (Complete - 75.1% mAP)\n",
    "   ‚úÖ Week 3: Medical Classifier (Complete - 94.48%)\n",
    "   ‚úÖ Day 22: Security System Planning (Complete)\n",
    "   ‚úÖ Day 23: DeepSORT Integration (Complete)\n",
    "   ‚úÖ Day 24: Tracking Optimization (Complete - TODAY!)\n",
    "   \n",
    "   Next: Video processing pipeline with multiple sources! üé¨\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0291db-a6db-43da-9e8b-5d603c63f502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
